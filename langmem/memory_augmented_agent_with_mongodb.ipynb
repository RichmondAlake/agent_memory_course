{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc42987",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/RichmondAlake/agent_memory_course/blob/main/information_retrieval/zero_to_hero_with_genai_with_mongodb_openai.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bcb92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langmem langgraph langchain_voyageai langgraph-checkpoint-mongodb langgraph-store-mongodb pymongo requests pypdf langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b51274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"VOYAGE_API_KEY\", \"Enter your Voyage API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc693319",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"VOYAGE_API_KEY\", \"Enter your Voyage API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69efb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI=os.environ[\"MONGODB_URI\"]\n",
    "DATABASE_NAME=\"langmem_agent_memory\"\n",
    "PROCEDURAL_MEMORY_COLLECTION_NAME=\"procedural_memory\"\n",
    "SEMANTIC_MEMORY_COLLECTION_NAME=\"semantic_memory\"\n",
    "STATE_CHECKPOINT_COLLECTION_NAME=\"state_checkpoints\"\n",
    "\n",
    "# Define namespaces for different types of memory\n",
    "USER_MEMORY_NAMESPACE = (\"user_memories\",)  # For user-specific memories\n",
    "KNOWLEDGE_NAMESPACE = (\"agent_memory_survey\",)  # For PDF content (matches our storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6df36a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richmondalake/miniconda3/envs/agent_memory_course/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langgraph.store.mongodb.base import MongoDBStore, VectorIndexConfig\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "procedural_collection = db[PROCEDURAL_MEMORY_COLLECTION_NAME]\n",
    "semantic_collection = db[SEMANTIC_MEMORY_COLLECTION_NAME]\n",
    "\n",
    "procedural_vector_index_config = VectorIndexConfig(\n",
    "    dims=1024,\n",
    "    index_name=\"procedural_memory_index\",\n",
    "    filters=None,\n",
    "    fields=None,\n",
    "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    ")\n",
    "\n",
    "semantic_vector_index_config = VectorIndexConfig(\n",
    "    dims=1024,\n",
    "    index_name=\"semantic_memory_index\",\n",
    "    filters=None,\n",
    "    fields=None,\n",
    "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8387788",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store = MongoDBStore(\n",
    "    collection=procedural_collection,\n",
    "    index_config=procedural_vector_index_config,\n",
    "    auto_index_timeout=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec885e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_memory_store = MongoDBStore(\n",
    "    collection=semantic_collection,\n",
    "    index_config=semantic_vector_index_config,\n",
    "    auto_index_timeout=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d8d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "checkpointer = MongoDBSaver(\n",
    "    client=client,\n",
    "    db_name=DATABASE_NAME, \n",
    "    collection_name=STATE_CHECKPOINT_COLLECTION_NAME,\n",
    "    index_config=procedural_memory_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05819448",
   "metadata": {},
   "source": [
    "## Data Ingestion and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435957f",
   "metadata": {},
   "source": [
    "Semantic Memory Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8459bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDF from https://arxiv.org/pdf/2404.13501...\n",
      "Loading PDF with LangChain...\n",
      "Chunking text...\n",
      "Successfully created 197 chunks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Method 1: Add the project root directory to Python path\n",
    "project_root = \"/Users/richmondalake/Desktop/Projects/open_source/agent_memory_course\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utilities.pdf_chunker import ingest_pdf_and_chunk\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2404.13501\"\n",
    "\n",
    "# Ingest and chunk the PDF\n",
    "chunks = ingest_pdf_and_chunk(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93390b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 197 chunks in procedural memory...\n",
      "All chunks stored successfully!\n",
      "\n",
      "First chunk example:\n",
      "Key: pdf_chunk_0\n",
      "Content preview: A Survey on the Memory Mechanism of Large\n",
      "Language Model based Agents\n",
      "Zeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\n",
      "Jieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\n",
      "1Gaoling School of Ar...\n"
     ]
    }
   ],
   "source": [
    "from utilities.pdf_chunker import store_chunks_in_memory\n",
    "\n",
    "# Store chunks in procedural memory\n",
    "store_chunks_in_memory(chunks, semantic_memory_store, KNOWLEDGE_NAMESPACE[0])\n",
    "\n",
    "# Print first chunk as example\n",
    "if chunks:\n",
    "    print(\"\\nFirst chunk example:\")\n",
    "    print(f\"Key: {chunks[0]['key']}\")\n",
    "    print(f\"Content preview: {chunks[0]['value']['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "331b897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": \"Write good paper summaries.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "517899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(state):\n",
    "    # Get procedural instructions\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    \n",
    "    # Get user query for semantic search\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search semantic memory for relevant content\n",
    "    knowledge_items = []\n",
    "    try:\n",
    "        print(\"Searching semantic memory for relevant content\")\n",
    "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=5)\n",
    "        print(knowledge_items)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
    "    \n",
    "    # Build system content with instructions and relevant knowledge\n",
    "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
    "    \n",
    "    if knowledge_items:\n",
    "        system_content += \"## Relevant Knowledge from Agent Memory Research:\\n\"\n",
    "        for item in knowledge_items:\n",
    "            content = item.value.get('content', str(item.value))[:500]  # Limit content length\n",
    "            system_content += f\"- {content}...\\n\"\n",
    "        system_content += \"\\nUse this knowledge to provide informed, accurate responses.\\n\\n\"\n",
    "    \n",
    "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
    "    return [sys_prompt] + state['messages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "840b6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prompt(state):\n",
    "    \"\"\"Simple prompt that only includes procedural instructions\"\"\"\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\\n\\nYou have access to search tools for knowledge retrieval when needed.\"}\n",
    "    return [sys_prompt] + state['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8004e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "\n",
    "# 🚨 The tools argument can be turned into toolbox memory, a form of procedural memory that can be used to store tools and their descriptions (Use BigTools)\n",
    "# Create memory management tools for different namespaces\n",
    "memory_tools = [\n",
    "    create_manage_memory_tool(USER_MEMORY_NAMESPACE),    # User-specific memories\n",
    "    create_search_memory_tool(USER_MEMORY_NAMESPACE),    # Search user memories  \n",
    "    create_search_memory_tool(KNOWLEDGE_NAMESPACE),      # Search PDF knowledge base\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd8cb2",
   "metadata": {},
   "source": [
    "Agent that uses memory tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d119a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    prompt=prompt, # Prompt for the agent obtained from the database (procedural memory)\n",
    "    tools=memory_tools,\n",
    "    store= procedural_memory_store, # Storing the semantic knowledge\n",
    "    checkpointer=checkpointer, # Storing the state of the agent in the database (procedural memory)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "233fca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(agent, query, thread_id, user_id=None, return_full_result=False):\n",
    "    \"\"\"\n",
    "    Enhanced chat function that supports user-specific memory and can return full results\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent to invoke\n",
    "        query: User message text\n",
    "        thread_id: Thread identifier for conversation continuity\n",
    "        user_id: Optional user identifier for personalized memory\n",
    "        return_full_result: If True, returns full result state; if False, returns just the content\n",
    "    \n",
    "    Returns:\n",
    "        If return_full_result=True: Full result state with all messages\n",
    "        If return_full_result=False: Just the last message content\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    if user_id:\n",
    "        config[\"configurable\"][\"user_id\"] = user_id\n",
    "    \n",
    "    result_state = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]}, \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if return_full_result:\n",
    "        return result_state\n",
    "    else:\n",
    "        return result_state[\"messages\"][-1].content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9801c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching semantic memory for relevant content\n",
      "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_145', value={'content': 'Acknowledgement\\nWe thank Lei Wang for his proofreading and valuable suggestions to this survey.\\nReferences\\n[1] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\\nand Maosong Sun. Communicative agents for software development. arXiv preprint\\narXiv:2307.07924, 2023.\\n[2] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng\\nJin, and Yong Li. S3: Social-network simulation system with large language model-empowered\\nagents. arXiv preprint arXiv:2307.14984, 2023.\\n[3] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\\nJiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous\\nagents. arXiv preprint arXiv:2308.11432, 2023.\\n28', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 145, 'total_chunks': 197}, created_at='2025-08-13T01:28:03.203000', updated_at='2025-08-13T01:28:03.203000', score=0.7557480931282043), Item(namespace=['agent_memory_survey'], key='pdf_chunk_29', value={'content': 'et al. [37], Xu et al. [38].\\nEvaluation Chang et al. [34], Guo et al. [35].\\nFundamental Problems\\nEfficiency\\nBai et al. [26], Wan et al. [27], Miao\\net al. [28, 28], Xu et al. [29], Zhu\\net al. [30], Xu and McAuley [31],\\nWang et al. [32], Park et al. [33].\\nMultimodal Wu et al. [22], Song et al. [23],\\nCaffagni et al. [24], Yin et al. [25].\\nLong-context Huang et al. [19], Wang\\net al. [20], Pawar et al. [21].\\nTool Usage Qin et al. [18].\\nKnowledge Editing\\nWang et al. [13], Yao et al.\\n[14], Wang et al. [15], Feng\\net al. [16], Zhang et al. [17].\\nRetrieval Augmentation Gao et al. [12].\\nAlignment Shen et al. [9], Wang\\net al. [10], Liu et al. [11].\\nSupervised Fine-tuning Zhang et al. [8].\\nLLM-based Agent\\n[3, 4, 77–80]\\nChallenges\\nApplications Li et al. [76].\\nEvaluation\\nFundamental Problems\\nMulti-agents Guo et al. [75].\\nMemory Our Survey.\\nPlanning Huang et al. [74].\\nFigure 2: The organization of related surveys on LLMs and LLM-based agents.\\n6', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 29, 'total_chunks': 197}, created_at='2025-08-13T01:27:30.582000', updated_at='2025-08-13T01:27:30.581000', score=0.7325276732444763), Item(namespace=['agent_memory_survey'], key='pdf_chunk_146', value={'content': '[4] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\n[5] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference\\non Neural Information Processing Systems, 2023.\\n[6] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large\\nlanguage models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.\\n[7] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards\\na general read-write memory for large language models. arXiv preprint arXiv:2305.14322,\\n2023.\\n[8] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\\nRunyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 146, 'total_chunks': 197}, created_at='2025-08-13T01:28:03.495000', updated_at='2025-08-13T01:28:03.495000', score=0.7309134602546692), Item(namespace=['agent_memory_survey'], key='pdf_chunk_148', value={'content': 'Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and\\nguideline for evaluating large language models’ alignment. arXiv preprint arXiv:2308.05374,\\n2023.\\n[12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\\nand Haofen Wang. Retrieval-augmented generation for large language models: A survey.\\narXiv preprint arXiv:2312.10997, 2023.\\n[13] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing\\nfor large language models: A survey. arXiv preprint arXiv:2310.16218, 2023.\\n[14] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun\\nChen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportuni-\\nties. arXiv preprint arXiv:2305.13172, 2023.\\n[15] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi,\\nSiyuan Cheng, Kangwei Liu, Guozhou Zheng, et al. Easyedit: An easy-to-use knowledge', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 148, 'total_chunks': 197}, created_at='2025-08-13T01:28:04.079000', updated_at='2025-08-13T01:28:04.079000', score=0.7307863235473633), Item(namespace=['agent_memory_survey'], key='pdf_chunk_28', value={'content': 'LLMs\\n[70–73]\\nChallenges\\nSecurity\\nYao et al. [64], Shayegani et al.\\n[65], Neel and Chang [66], Smith et al.\\n[67], Dong et al. [68], Das et al. [69].\\nExplainability Zhao et al. [63].\\nBias & Fairness Gallegos et al. [60], Kotek\\net al. [61], Li et al. [62].\\nHallucination\\nZhang et al. [53], Huang et al.\\n[54], Rawte et al. [55], Ye\\net al. [56], Ji et al. [57], Ton-\\nmoy et al. [58], Jiang et al. [59].\\nApplications\\nRecommendation Li et al. [50], Lin et al.\\n[51], Wang et al. [52].\\nPsychology He et al. [49].\\nFinance Li et al. [48].\\nMedecine He et al. [45], Zhou et al.\\n[46], Wang et al. [47].\\nAutonomous Driving Cui et al. [43], Yang et al. [44].\\nRobotics Zeng et al. [42].\\nSoftware Engineering Fan et al. [39], Wang et al.\\n[40], Zheng et al. [41].\\nInformation Processing Yang et al. [36], Zhu\\net al. [37], Xu et al. [38].\\nEvaluation Chang et al. [34], Guo et al. [35].\\nFundamental Problems\\nEfficiency\\nBai et al. [26], Wan et al. [27], Miao\\net al. [28, 28], Xu et al. [29], Zhu', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 28, 'total_chunks': 197}, created_at='2025-08-13T01:27:30.261000', updated_at='2025-08-13T01:27:30.261000', score=0.7296693325042725)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = chat_with_agent(\n",
    "    agent=agent, \n",
    "    query=\"Who are the authors of the paper?\",\n",
    "    thread_id=\"10\",\n",
    "    user_id=\"user-123\",\n",
    "    return_full_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "061b6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper titled \"Communicative agents for software development\" is authored by Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "134d6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_prompt_optimizer\n",
    "\n",
    "optimizer = create_prompt_optimizer(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f86fd",
   "metadata": {},
   "source": [
    "Understand and explain trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5800a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompt = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\").value[\"prompt\"]\n",
    "feedback = {\"request\": \"Always respond with sentences starting with, according to the paper in question...\"}\n",
    "\n",
    "optimizer_result = optimizer.invoke({\"prompt\": current_prompt, \"trajectories\": [(response[\"messages\"], feedback)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ce5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current prompt: Write good paper summaries.\n",
      "Optimizer result: Write good paper summaries. For each factual statement drawn from a paper, preface it with 'According to the paper...' to ensure clarity and consistency in style.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current prompt: {current_prompt}\")\n",
    "print(f\"Optimizer result: {optimizer_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c2c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": optimizer_result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6ff1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching semantic memory for relevant content\n",
      "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_20', value={'content': 'their key contributions. Then, we discuss the problems of “what is”, “why do we need” and “how\\nto implement and evaluate” the memory module in LLM-based agents in Section 3 to 6. Next, we\\nshow the applications of memory-enhanced agents in Section 7. The discussions of the limitations of\\nexisting work and future directions come at last in Section 8 and Section 9.\\n4', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 20, 'total_chunks': 197}, created_at='2025-08-13T01:27:27.751000', updated_at='2025-08-13T01:27:27.751000', score=0.7392587065696716), Item(namespace=['agent_memory_survey'], key='pdf_chunk_22', value={'content': 'which, however, provide different taxonomies and understandings on LLMs. Following these surveys,\\npeople dive into specific aspects of LLMs and review the corresponding milestone studies and key\\ntechnologies. These aspects can be classified into four categories including the fundamental problems,\\nevaluation, applications, and challenges of LLMs.\\nFundamental problems. The surveys in this category aim to summarize techniques that can\\nbe leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\\ncomprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\\ntraining LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\\nLLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 22, 'total_chunks': 197}, created_at='2025-08-13T01:27:28.301000', updated_at='2025-08-13T01:27:28.301000', score=0.7374104261398315), Item(namespace=['agent_memory_survey'], key='pdf_chunk_82', value={'content': 'The fine-tuning methods can effectively bridge the gap between general agents and specialized\\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies\\nin the computational cost and time consumption, as well as the requirement of a large amount of\\ndata. Therefore, most fine-tuning approaches are applied to offline scenarios, and can seldom deal\\nwith online scenarios, such as fine-tuning with agent observations and trial experiences. Due to\\nthe frequent agent-environment interactions, it is unaffordable for the cost of backpropagation to\\nfine-tune every step of the online and dynamic interactions.', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 82, 'total_chunks': 197}, created_at='2025-08-13T01:27:45.745000', updated_at='2025-08-13T01:27:45.745000', score=0.7344887256622314), Item(namespace=['agent_memory_survey'], key='pdf_chunk_50', value={'content': 'memory is widely recognized as an extremely important one [84]. It is fundamental for humans to\\nlearn knowledge by accumulating important information and abstracting high-level concepts [85],\\nform social norms by remembering cultural values and individual experiences [86], take reasonable\\nbehaviors by imagining the potential positive and negative consequences [87], and among others.\\nA major goal of LLM-based agents is to replace humans for accomplishing different tasks. To make\\nagents behave like humans, following human’s working mechanisms to design the agents is a natural\\nand essential choice [88]. Since memory is important for humans, designing memory modules is also\\nsignificant for the agents. In addition, cognitive psychology has been studied for a long time, so many\\neffective human memory theories and architectures have been accumulated, which can support more\\nadvanced capabilities of the agents [89].\\n2https://en.wikipedia.org/wiki/Cognitive_psychology\\n10', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 50, 'total_chunks': 197}, created_at='2025-08-13T01:27:36.430000', updated_at='2025-08-13T01:27:36.430000', score=0.7338302135467529), Item(namespace=['agent_memory_survey'], key='pdf_chunk_16', value={'content': 'informative knowledge to support its actions, and so on. Around the memory module, people have\\ndevoted much effort to designing its information sources, storage forms, and operation mechanisms.\\nFor example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory\\nmodule for enhancing the agent’s reasoning capability. Zhong et al. [6] store memory information in\\nthe form of natural languages, which is explainable and friendly to the users. Modarressi et al. [7]\\ndesign both memory reading and writing operations to interact with environments for task solving.\\nWhile previous studies have designed many promising memory modules, there still lacks a systemic\\nstudy to view the memory modules from a holistic perspective. To bridge this gap, in this paper,\\nwe comprehensively review previous studies to present clear taxonomies and key principles for\\ndesigning and evaluating the memory module. In specific, we discuss three key problems including:', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 16, 'total_chunks': 197}, created_at='2025-08-13T01:27:26.669000', updated_at='2025-08-13T01:27:26.669000', score=0.7322878837585449)]\n"
     ]
    }
   ],
   "source": [
    "response = chat_with_agent(\n",
    "    agent,\n",
    "    \"What is the main point of the paper?\",\n",
    "    \"0\",\n",
    "    \"user-123\",\n",
    "    return_full_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8974dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_prompt(state):\n",
    "    \"\"\"Prompt with eager retrieval - no need for store parameter\"\"\"\n",
    "    # Get procedural instructions\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    \n",
    "    # Get user query for semantic search\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Direct access to semantic memory (no need for store parameter)\n",
    "    knowledge_items = []\n",
    "    try:\n",
    "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=3)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
    "    \n",
    "    # Build system content\n",
    "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
    "    \n",
    "    if knowledge_items:\n",
    "        system_content += \"## Relevant Knowledge:\\n\"\n",
    "        for item in knowledge_items:\n",
    "            content = item.value.get('content', str(item.value))[:400]\n",
    "            system_content += f\"- {content}...\\n\"\n",
    "        system_content += \"\\n\"\n",
    "    \n",
    "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
    "    return [sys_prompt] + state['messages']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9eef7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with eager retrieval (no store parameter needed for semantic memory)\n",
    "eager_agent = create_react_agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    prompt=eager_prompt,  # Automatic knowledge injection\n",
    "    tools=[],  # No search tools needed\n",
    "    store=procedural_memory_store,  # Only procedural memory in store\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff6135e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\n",
      "Response: According to the paper, the authors are Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\")\n",
    "query = \"Who are the authors of the paper?\"\n",
    "response2 = chat_with_agent(eager_agent, query, \"eager-test\", return_full_result=False)\n",
    "print(f\"Response: {response2[:200]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install BigTool\n",
    "%pip install -q langgraph-bigtool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12 memory tools:\n",
      "  - search_user_memories: Search store and retrieve user-specific information, preferences, and personal data\n",
      "  - manage_user_memories: Store/update information in store and retrieve user-specific information, preferences, and personal data\n",
      "  - search_agent_memory_survey: Search search the comprehensive research paper about agent memory mechanisms\n",
      "  - manage_agent_memory_survey: Store/update information in search the comprehensive research paper about agent memory mechanisms\n",
      "  - search_conversations: Search store and retrieve conversation history and context\n",
      "  - manage_conversations: Store/update information in store and retrieve conversation history and context\n",
      "  - search_user_preferences: Search manage user settings, preferences, and configuration\n",
      "  - manage_user_preferences: Store/update information in manage user settings, preferences, and configuration\n",
      "  - search_project_knowledge: Search store and retrieve project-specific knowledge and documentation\n",
      "  - manage_project_knowledge: Store/update information in store and retrieve project-specific knowledge and documentation\n",
      "  - search_research_papers: Search general research paper storage and retrieval\n",
      "  - manage_research_papers: Store/update information in general research paper storage and retrieval\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import Literal, Dict, Any\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from langgraph_bigtool import create_agent\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "# Define available memory namespaces and their purposes\n",
    "MEMORY_NAMESPACES = {\n",
    "    \"user_memories\": \"Store and retrieve user-specific information, preferences, and personal data\",\n",
    "    \"agent_memory_survey\": \"Search the comprehensive research paper about agent memory mechanisms\",\n",
    "    \"conversations\": \"Store and retrieve conversation history and context\",\n",
    "    \"user_preferences\": \"Manage user settings, preferences, and configuration\",\n",
    "    \"project_knowledge\": \"Store and retrieve project-specific knowledge and documentation\",\n",
    "    \"research_papers\": \"General research paper storage and retrieval\",\n",
    "}\n",
    "\n",
    "def create_dynamic_memory_tool_registry() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a registry of memory tools that can be dynamically retrieved.\n",
    "    This scales much better than having all tools in an array.\n",
    "    \"\"\"\n",
    "    tool_registry = {}\n",
    "    \n",
    "    # Create search and manage tools for each namespace\n",
    "    for namespace, description in MEMORY_NAMESPACES.items():\n",
    "        namespace_tuple = (namespace,)\n",
    "        \n",
    "        # Search tool for this namespace\n",
    "        search_tool = create_search_memory_tool(namespace_tuple)\n",
    "        search_tool.description = f\"Search {description.lower()}\"\n",
    "        search_id = f\"search_{namespace}\"\n",
    "        tool_registry[search_id] = search_tool\n",
    "        \n",
    "        # Manage tool for this namespace  \n",
    "        manage_tool = create_manage_memory_tool(namespace_tuple)\n",
    "        manage_tool.description = f\"Store/update information in {description.lower()}\"\n",
    "        manage_id = f\"manage_{namespace}\"\n",
    "        tool_registry[manage_id] = manage_tool\n",
    "    \n",
    "    return tool_registry\n",
    "\n",
    "# Create the tool registry\n",
    "memory_tool_registry = create_dynamic_memory_tool_registry()\n",
    "\n",
    "print(f\"Created {len(memory_tool_registry)} memory tools:\")\n",
    "for tool_id, tool in memory_tool_registry.items():\n",
    "    print(f\"  - {tool_id}: {tool.description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory tools indexed for semantic retrieval\n"
     ]
    }
   ],
   "source": [
    "# Index memory tools in a store for semantic retrieval\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# Create a separate store for tool indexing\n",
    "tool_store = MongoDBStore(\n",
    "    collection=db[\"memory_tools\"],\n",
    "    index_config=VectorIndexConfig(\n",
    "        dims=1024,\n",
    "        index_name=\"memory_tools_index\", \n",
    "        embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    "        filters=None,\n",
    "        fields=None,\n",
    "        auto_index_timeout=70\n",
    "    )\n",
    ")\n",
    "\n",
    "# Index each tool with its description for semantic search\n",
    "for tool_id, tool in memory_tool_registry.items():\n",
    "    tool_store.put(\n",
    "        (\"memory_tools\",),\n",
    "        tool_id,\n",
    "        {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"namespace\": tool_id.split(\"_\", 1)[1],  # Extract namespace from ID\n",
    "            \"operation\": tool_id.split(\"_\", 1)[0],  # Extract operation (search/manage)\n",
    "        },\n",
    "    )\n",
    "\n",
    "print(\"Memory tools indexed for semantic retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tool retrieval function created\n"
     ]
    }
   ],
   "source": [
    "def retrieve_memory_tools(\n",
    "    query: str,\n",
    "    operation_type: Literal[\"search\", \"manage\", \"both\"] = \"both\",\n",
    "    *,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Intelligently retrieve memory tools based on query and operation type.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query to determine relevant memory namespaces\n",
    "        operation_type: Whether to retrieve search tools, manage tools, or both\n",
    "        store: The injected tool store for searching\n",
    "    \n",
    "    Returns:\n",
    "        List of tool IDs that are relevant to the query\n",
    "    \"\"\"\n",
    "    # Search for relevant tools based on the query\n",
    "    results = store.search((\"memory_tools\",), query=query, limit=6)\n",
    "    \n",
    "    # Filter by operation type if specified\n",
    "    tool_ids = []\n",
    "    for result in results:\n",
    "        tool_id = result.key\n",
    "        tool_data = result.value\n",
    "        \n",
    "        if operation_type == \"both\":\n",
    "            tool_ids.append(tool_id)\n",
    "        elif operation_type == \"search\" and tool_data[\"operation\"] == \"search\":\n",
    "            tool_ids.append(tool_id)\n",
    "        elif operation_type == \"manage\" and tool_data[\"operation\"] == \"manage\":\n",
    "            tool_ids.append(tool_id)\n",
    "    \n",
    "    # Ensure we always include the research paper search tool for research questions\n",
    "    research_indicators = [\"paper\", \"research\", \"author\", \"study\", \"survey\", \"memory mechanism\"]\n",
    "    if any(indicator in query.lower() for indicator in research_indicators):\n",
    "        if \"search_agent_memory_survey\" not in tool_ids:\n",
    "            tool_ids.insert(0, \"search_agent_memory_survey\")\n",
    "    \n",
    "    # Limit to avoid overwhelming the LLM\n",
    "    return tool_ids[:4]\n",
    "\n",
    "print(\"Custom tool retrieval function created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_memory_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
