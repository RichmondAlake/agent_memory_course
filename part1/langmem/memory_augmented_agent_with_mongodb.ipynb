{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc42987",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichmondAlake/agent_memory_course/blob/main/langmem/memory_augmented_agent_with_mongodb.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bcb92c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(5012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU langmem langgraph langchain_voyageai langgraph-checkpoint-mongodb langgraph-store-mongodb pymongo requests pypdf langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b51274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c887a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"MONGODB_URI\", \"Enter your MongoDB URI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ffb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"VOYAGE_API_KEY\", \"Enter your Voyage API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc693319",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69efb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGODB_URI=os.environ[\"MONGODB_URI\"]\n",
    "DATABASE_NAME=\"langmem_agent_memory\"\n",
    "PROCEDURAL_MEMORY_COLLECTION_NAME=\"procedural_memory\"\n",
    "SEMANTIC_MEMORY_COLLECTION_NAME=\"semantic_memory\"\n",
    "STATE_CHECKPOINT_COLLECTION_NAME=\"state_checkpoints\"\n",
    "\n",
    "# Define namespaces for different types of memory\n",
    "USER_MEMORY_NAMESPACE = (\"user_memories\",)  # For user-specific memories\n",
    "KNOWLEDGE_NAMESPACE = (\"agent_memory_survey\",)  # For PDF content (matches our storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6df36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "from langgraph.store.mongodb.base import MongoDBStore, VectorIndexConfig\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[DATABASE_NAME]\n",
    "procedural_collection = db[PROCEDURAL_MEMORY_COLLECTION_NAME]\n",
    "semantic_collection = db[SEMANTIC_MEMORY_COLLECTION_NAME]\n",
    "\n",
    "procedural_vector_index_config = VectorIndexConfig(\n",
    "    dims=1024,\n",
    "    index_name=\"procedural_memory_index\",\n",
    "    filters=None,\n",
    "    fields=None,\n",
    "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    ")\n",
    "\n",
    "semantic_vector_index_config = VectorIndexConfig(\n",
    "    dims=1024,\n",
    "    index_name=\"semantic_memory_index\",\n",
    "    filters=None,\n",
    "    fields=None,\n",
    "    embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8387788",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store = MongoDBStore(\n",
    "    collection=procedural_collection,\n",
    "    index_config=procedural_vector_index_config,\n",
    "    auto_index_timeout=70\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec885e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_memory_store = MongoDBStore(\n",
    "    collection=semantic_collection,\n",
    "    index_config=semantic_vector_index_config,\n",
    "    auto_index_timeout=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22d8d308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "\n",
    "checkpointer = MongoDBSaver(\n",
    "    client=client,\n",
    "    db_name=DATABASE_NAME, \n",
    "    collection_name=STATE_CHECKPOINT_COLLECTION_NAME,\n",
    "    index_config=procedural_memory_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05819448",
   "metadata": {},
   "source": [
    "## Data Ingestion and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435957f",
   "metadata": {},
   "source": [
    "Semantic Memory Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8459bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PDF from https://arxiv.org/pdf/2404.13501...\n",
      "Loading PDF with LangChain...\n",
      "Chunking text...\n",
      "Successfully created 197 chunks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Method 1: Add the project root directory to Python path\n",
    "project_root = \"/Users/richmondalake/Desktop/Projects/open_source/agent_memory_course\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utilities.pdf_chunker import ingest_pdf_and_chunk\n",
    "\n",
    "url = \"https://arxiv.org/pdf/2404.13501\"\n",
    "\n",
    "# Ingest and chunk the PDF\n",
    "chunks = ingest_pdf_and_chunk(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93390b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 197 chunks in procedural memory...\n",
      "All chunks stored successfully!\n",
      "\n",
      "First chunk example:\n",
      "Key: pdf_chunk_0\n",
      "Content preview: A Survey on the Memory Mechanism of Large\n",
      "Language Model based Agents\n",
      "Zeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\n",
      "Jieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\n",
      "1Gaoling School of Ar...\n"
     ]
    }
   ],
   "source": [
    "from utilities.pdf_chunker import store_chunks_in_memory\n",
    "\n",
    "# Store chunks in procedural memory\n",
    "store_chunks_in_memory(chunks, semantic_memory_store, KNOWLEDGE_NAMESPACE[0])\n",
    "\n",
    "# Print first chunk as example\n",
    "if chunks:\n",
    "    print(\"\\nFirst chunk example:\")\n",
    "    print(f\"Key: {chunks[0]['key']}\")\n",
    "    print(f\"Content preview: {chunks[0]['value']['content'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "331b897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": \"Write good paper summaries.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(state):\n",
    "    # Get procedural instructions\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    \n",
    "    # Get user query for semantic search\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Search semantic memory for relevant content\n",
    "    knowledge_items = []\n",
    "    try:\n",
    "        print(\"Searching semantic memory for relevant content\")\n",
    "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=5)\n",
    "        print(knowledge_items)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
    "    \n",
    "    # Build system content with instructions and relevant knowledge\n",
    "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
    "    \n",
    "    if knowledge_items:\n",
    "        system_content += \"## Relevant Knowledge from Agent Memory Research:\\n\"\n",
    "        for item in knowledge_items:\n",
    "            content = item.value.get('content', str(item.value))[:500]  # Limit content length\n",
    "            system_content += f\"- {content}...\\n\"\n",
    "        system_content += \"\\nUse this knowledge to provide informed, accurate responses.\\n\\n\"\n",
    "    \n",
    "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
    "    return [sys_prompt] + state['messages']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840b6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prompt(state):\n",
    "    \"\"\"Simple prompt that only includes procedural instructions\"\"\"\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    sys_prompt = {\"role\": \"system\", \"content\": f\"## Instructions\\n\\n{instructions}\\n\\nYou have access to search tools for knowledge retrieval when needed.\"}\n",
    "    return [sys_prompt] + state['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8004e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "\n",
    "# ðŸš¨ The tools argument can be turned into toolbox memory, a form of procedural memory that can be used to store tools and their descriptions (Use BigTools)\n",
    "# Create memory management tools for different namespaces\n",
    "memory_tools = [\n",
    "    create_manage_memory_tool(USER_MEMORY_NAMESPACE),    # User-specific memories\n",
    "    create_search_memory_tool(USER_MEMORY_NAMESPACE),    # Search user memories  \n",
    "    create_search_memory_tool(KNOWLEDGE_NAMESPACE),      # Search PDF knowledge base\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfd8cb2",
   "metadata": {},
   "source": [
    "Agent that uses memory tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d119a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    prompt=prompt, # Prompt for the agent obtained from the database (procedural memory)\n",
    "    tools=memory_tools,\n",
    "    store= procedural_memory_store, # Storing the semantic knowledge\n",
    "    checkpointer=checkpointer, # Storing the state of the agent in the database (procedural memory)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "233fca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_agent(agent, query, thread_id, user_id=None, return_full_result=False):\n",
    "    \"\"\"\n",
    "    Enhanced chat function that supports user-specific memory and can return full results\n",
    "    \n",
    "    Args:\n",
    "        agent: The agent to invoke\n",
    "        query: User message text\n",
    "        thread_id: Thread identifier for conversation continuity\n",
    "        user_id: Optional user identifier for personalized memory\n",
    "        return_full_result: If True, returns full result state; if False, returns just the content\n",
    "    \n",
    "    Returns:\n",
    "        If return_full_result=True: Full result state with all messages\n",
    "        If return_full_result=False: Just the last message content\n",
    "    \"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    if user_id:\n",
    "        config[\"configurable\"][\"user_id\"] = user_id\n",
    "    \n",
    "    result_state = agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": query}]}, \n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    if return_full_result:\n",
    "        return result_state\n",
    "    else:\n",
    "        return result_state[\"messages\"][-1].content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9801c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching semantic memory for relevant content\n",
      "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_0', value={'content': 'A Survey on the Memory Mechanism of Large\\nLanguage Model based Agents\\nZeyu Zhang1, Xiaohe Bo1, Chen Ma1, Rui Li1, Xu Chen1, Quanyu Dai2,\\nJieming Zhu2, Zhenhua Dong2, Ji-Rong Wen1\\n1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\n2Huawei Noahâ€™s Ark Lab, China\\nzeyuzhang@ruc.edu.cn, xu.chen@ruc.edu.cn\\nAbstract\\nLarge language model (LLM) based agents have recently attracted much attention\\nfrom the research and industry communities. Compared with original LLMs, LLM-\\nbased agents are featured in their self-evolving capability, which is the basis for\\nsolving real-world problems that need long-term and complex agent-environment\\ninteractions. The key component to support agent-environment interactions is the\\nmemory of the agents. While previous studies have proposed many promising mem-\\nory mechanisms, they are scattered in different papers, and there lacks a systemati-\\ncal review to summarize and compare these works from a holistic perspective, fail-', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 0, 'total_chunks': 197}, created_at='2025-08-13T19:13:05.764000', updated_at='2025-08-13T19:13:05.764000', score=0.8757513761520386), Item(namespace=['agent_memory_survey'], key='pdf_chunk_146', value={'content': '[4] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model\\nbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\n[5] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning. In Thirty-seventh Conference\\non Neural Information Processing Systems, 2023.\\n[6] Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. Memorybank: Enhancing large\\nlanguage models with long-term memory. arXiv preprint arXiv:2305.10250, 2023.\\n[7] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich SchÃ¼tze. Ret-llm: Towards\\na general read-write memory for large language models. arXiv preprint arXiv:2305.14322,\\n2023.\\n[8] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\\nRunyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 146, 'total_chunks': 197}, created_at='2025-08-13T19:13:48.994000', updated_at='2025-08-13T19:13:48.994000', score=0.863787055015564), Item(namespace=['agent_memory_survey'], key='pdf_chunk_173', value={'content': '[96] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb:\\nAugmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901,\\n2023.\\n[97] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang.\\nThink-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv\\npreprint arXiv:2311.08719, 2023.\\n[98] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and\\nZhoujun Li. Unleashing infinite-length input capacity for large-scale language models with\\nself-controlled memory system. arXiv preprint arXiv:2304.13343, 2023.\\n[99] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\\nFan, and Anima Anandkumar. V oyager: An open-ended embodied agent with large language\\nmodels. arXiv preprint arXiv:2305.16291, 2023.\\n[100] Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 173, 'total_chunks': 197}, created_at='2025-08-13T19:13:57.064000', updated_at='2025-08-13T19:13:57.064000', score=0.8594695925712585), Item(namespace=['agent_memory_survey'], key='pdf_chunk_19', value={'content': 'ory module and comprehensively analyze its necessity for LLM-based agents. (2) We systematically\\nsummarize existing studies on designing and evaluating the memory module in LLM-based agents,\\nproviding clear taxonomies and intuitive insights. (3) We present typical agent applications to show\\nthe importance of the memory module in different scenarios. (4) We analyze the key limitations of\\nexisting memory modules and show potential solutions for inspiring future studies. To our knowledge,\\nthis is the first survey on the memory mechanism of LLM-based agents.\\nThe rest of this survey is organized as follows. First, we provide a systematical meta-survey for the\\nfields of LLMs and LLM-based agents in Section 2, categorizing different surveys and summarizing\\ntheir key contributions. Then, we discuss the problems of â€œwhat isâ€, â€œwhy do we needâ€ and â€œhow\\nto implement and evaluateâ€ the memory module in LLM-based agents in Section 3 to 6. Next, we', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 19, 'total_chunks': 197}, created_at='2025-08-13T19:13:11.231000', updated_at='2025-08-13T19:13:11.231000', score=0.8508570790290833), Item(namespace=['agent_memory_survey'], key='pdf_chunk_8', value={'content': 'Contents\\n1 Introduction 4\\n2 Related Surveys 5\\n2.1 Surveys on Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Surveys on Large Language Model-based Agents . . . . . . . . . . . . . . . . . . 7\\n3 What is the Memory of LLM-based Agent 7\\n3.1 Basic Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3.2 Narrow Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 Broad Definition of the Agent Memory . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.4 Memory-assisted Agent-Environment Interaction . . . . . . . . . . . . . . . . . . 9\\n4 Why We Need the Memory in LLM-based Agent 10\\n4.1 Perspective of Cognitive Psychology . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n4.2 Perspective of Self-Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n4.3 Perspective of Agent Applications . . . . . . . . . . . . . . . . . . . . . . . . . . 11', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 8, 'total_chunks': 197}, created_at='2025-08-13T19:13:08.052000', updated_at='2025-08-13T19:13:08.052000', score=0.8504928350448608)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = chat_with_agent(\n",
    "    agent=agent, \n",
    "    query=\"Who are the authors of the paper A survey on the memory mechanism of large language models?\",\n",
    "    thread_id=\"10\",\n",
    "    user_id=\"user-123\",\n",
    "    return_full_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "061b6d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the paper \"A Survey on the Memory Mechanism of Large Language Model based Agents\" are Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "134d6f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_prompt_optimizer\n",
    "\n",
    "optimizer = create_prompt_optimizer(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f86fd",
   "metadata": {},
   "source": [
    "Understand and explain trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5800a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prompt = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\").value[\"prompt\"]\n",
    "feedback = {\"request\": \"Always respond with sentences starting with, according to the paper in question...\"}\n",
    "\n",
    "optimizer_result = optimizer.invoke({\"prompt\": current_prompt, \"trajectories\": [(response[\"messages\"], feedback)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51ce5ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current prompt: Write good paper summaries.\n",
      "Optimizer result: Write good paper summaries. Begin each summary with the phrase 'According to the paper in question...' and ensure that the response adheres to a specific structure and style, providing a clear and concise overview of key findings and implications.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current prompt: {current_prompt}\")\n",
    "print(f\"Optimizer result: {optimizer_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5c2c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procedural_memory_store.put((\"instructions\",), key=\"agent_instructions\", value={\"prompt\": optimizer_result})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ff1efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching semantic memory for relevant content\n",
      "[Item(namespace=['agent_memory_survey'], key='pdf_chunk_20', value={'content': 'their key contributions. Then, we discuss the problems of â€œwhat isâ€, â€œwhy do we needâ€ and â€œhow\\nto implement and evaluateâ€ the memory module in LLM-based agents in Section 3 to 6. Next, we\\nshow the applications of memory-enhanced agents in Section 7. The discussions of the limitations of\\nexisting work and future directions come at last in Section 8 and Section 9.\\n4', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 20, 'total_chunks': 197}, created_at='2025-08-13T19:13:11.495000', updated_at='2025-08-13T19:13:11.495000', score=0.7392587065696716), Item(namespace=['agent_memory_survey'], key='pdf_chunk_22', value={'content': 'which, however, provide different taxonomies and understandings on LLMs. Following these surveys,\\npeople dive into specific aspects of LLMs and review the corresponding milestone studies and key\\ntechnologies. These aspects can be classified into four categories including the fundamental problems,\\nevaluation, applications, and challenges of LLMs.\\nFundamental problems. The surveys in this category aim to summarize techniques that can\\nbe leveraged to tackle fundamental problems of LLMs. Specifically, Zhang et al. [8] provide a\\ncomprehensive survey on the methods of supervised fine-tuning, which is a key technique for better\\ntraining LLMs. Shen et al. [9], Wang et al.[10] and Liu et al. [11] present surveys on the alignment of\\nLLMs, which is a key requirement for LLMs to produce outputs consistent with human values. Gao\\net al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 22, 'total_chunks': 197}, created_at='2025-08-13T19:13:12.063000', updated_at='2025-08-13T19:13:12.063000', score=0.7374104261398315), Item(namespace=['agent_memory_survey'], key='pdf_chunk_82', value={'content': 'The fine-tuning methods can effectively bridge the gap between general agents and specialized\\nagents. It improves the capability of agents on the tasks that require high accuracy and reliability on\\ndomain-specific information. Nevertheless, fine-tuning LLMs for specific domains could potentially\\nlead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget\\nthe original knowledge because of updating their parameters. Another limitation of fine-tuning lies\\nin the computational cost and time consumption, as well as the requirement of a large amount of\\ndata. Therefore, most fine-tuning approaches are applied to offline scenarios, and can seldom deal\\nwith online scenarios, such as fine-tuning with agent observations and trial experiences. Due to\\nthe frequent agent-environment interactions, it is unaffordable for the cost of backpropagation to\\nfine-tune every step of the online and dynamic interactions.', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 82, 'total_chunks': 197}, created_at='2025-08-13T19:13:29.970000', updated_at='2025-08-13T19:13:29.970000', score=0.7344887256622314), Item(namespace=['agent_memory_survey'], key='pdf_chunk_50', value={'content': 'memory is widely recognized as an extremely important one [84]. It is fundamental for humans to\\nlearn knowledge by accumulating important information and abstracting high-level concepts [85],\\nform social norms by remembering cultural values and individual experiences [86], take reasonable\\nbehaviors by imagining the potential positive and negative consequences [87], and among others.\\nA major goal of LLM-based agents is to replace humans for accomplishing different tasks. To make\\nagents behave like humans, following humanâ€™s working mechanisms to design the agents is a natural\\nand essential choice [88]. Since memory is important for humans, designing memory modules is also\\nsignificant for the agents. In addition, cognitive psychology has been studied for a long time, so many\\neffective human memory theories and architectures have been accumulated, which can support more\\nadvanced capabilities of the agents [89].\\n2https://en.wikipedia.org/wiki/Cognitive_psychology\\n10', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 50, 'total_chunks': 197}, created_at='2025-08-13T19:13:20.291000', updated_at='2025-08-13T19:13:20.291000', score=0.7338302135467529), Item(namespace=['agent_memory_survey'], key='pdf_chunk_16', value={'content': 'informative knowledge to support its actions, and so on. Around the memory module, people have\\ndevoted much effort to designing its information sources, storage forms, and operation mechanisms.\\nFor example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory\\nmodule for enhancing the agentâ€™s reasoning capability. Zhong et al. [6] store memory information in\\nthe form of natural languages, which is explainable and friendly to the users. Modarressi et al. [7]\\ndesign both memory reading and writing operations to interact with environments for task solving.\\nWhile previous studies have designed many promising memory modules, there still lacks a systemic\\nstudy to view the memory modules from a holistic perspective. To bridge this gap, in this paper,\\nwe comprehensively review previous studies to present clear taxonomies and key principles for\\ndesigning and evaluating the memory module. In specific, we discuss three key problems including:', 'source': 'https://arxiv.org/pdf/2404.13501', 'chunk_index': 16, 'total_chunks': 197}, created_at='2025-08-13T19:13:10.377000', updated_at='2025-08-13T19:13:10.377000', score=0.7322878837585449)]\n"
     ]
    }
   ],
   "source": [
    "response = chat_with_agent(\n",
    "    agent,\n",
    "    \"What is the main point of the paper?\",\n",
    "    \"0\",\n",
    "    \"user-123\",\n",
    "    return_full_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dbdbe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the paper in question, the main point is to explore the integration of memory modules in LLM (Large Language Model)-based agents. The paper dives into the fundamental questions of what memory modules are, why they are necessary, and how to implement and evaluate them within LLM-based agents. The focus is on enhancing these agents' reasoning and decision-making capabilities by leveraging memory. It also provides insights into various applications of memory-enhanced agents, discusses the limitations of current approaches, and suggests future research directions. Ultimately, the paper underlines the importance of memory in making LLM-based agents more effective in replacing human-like task execution.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8974dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_prompt(state):\n",
    "    \"\"\"Prompt with eager retrieval - no need for store parameter\"\"\"\n",
    "    # Get procedural instructions\n",
    "    item = procedural_memory_store.get((\"instructions\",), key=\"agent_instructions\")\n",
    "    instructions = item.value[\"prompt\"]\n",
    "    \n",
    "    # Get user query for semantic search\n",
    "    user_query = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Direct access to semantic memory (no need for store parameter)\n",
    "    knowledge_items = []\n",
    "    try:\n",
    "        knowledge_items = semantic_memory_store.search(KNOWLEDGE_NAMESPACE, query=user_query, limit=3)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not search semantic memory: {e}\")\n",
    "    \n",
    "    # Build system content\n",
    "    system_content = f\"## Instructions\\n\\n{instructions}\\n\\n\"\n",
    "    \n",
    "    if knowledge_items:\n",
    "        system_content += \"## Relevant Knowledge:\\n\"\n",
    "        for item in knowledge_items:\n",
    "            content = item.value.get('content', str(item.value))[:400]\n",
    "            system_content += f\"- {content}...\\n\"\n",
    "        system_content += \"\\n\"\n",
    "    \n",
    "    sys_prompt = {\"role\": \"system\", \"content\": system_content}\n",
    "    return [sys_prompt] + state['messages']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9eef7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with eager retrieval (no store parameter needed for semantic memory)\n",
    "eager_agent = create_react_agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    prompt=eager_prompt,  # Automatic knowledge injection\n",
    "    tools=[],  # No search tools needed\n",
    "    store=procedural_memory_store,  # Only procedural memory in store\n",
    "    checkpointer=checkpointer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff6135e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\n",
      "Response: According to the paper, the authors are Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Approach 2: Eager Retrieval (Always includes relevant knowledge) ===\")\n",
    "query = \"Who are the authors of the paper?\"\n",
    "response2 = chat_with_agent(eager_agent, query, \"eager-test\", return_full_result=False)\n",
    "print(f\"Response: {response2[:200]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install BigTool\n",
    "%pip install -q langgraph-bigtool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 12 memory tools:\n",
      "  - search_user_memories: Search store and retrieve user-specific information, preferences, and personal data\n",
      "  - manage_user_memories: Store/update information in store and retrieve user-specific information, preferences, and personal data\n",
      "  - search_agent_memory_survey: Search search the comprehensive research paper about agent memory mechanisms\n",
      "  - manage_agent_memory_survey: Store/update information in search the comprehensive research paper about agent memory mechanisms\n",
      "  - search_conversations: Search store and retrieve conversation history and context\n",
      "  - manage_conversations: Store/update information in store and retrieve conversation history and context\n",
      "  - search_user_preferences: Search manage user settings, preferences, and configuration\n",
      "  - manage_user_preferences: Store/update information in manage user settings, preferences, and configuration\n",
      "  - search_project_knowledge: Search store and retrieve project-specific knowledge and documentation\n",
      "  - manage_project_knowledge: Store/update information in store and retrieve project-specific knowledge and documentation\n",
      "  - search_research_papers: Search general research paper storage and retrieval\n",
      "  - manage_research_papers: Store/update information in general research paper storage and retrieval\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from typing import Literal, Dict, Any\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "from langgraph_bigtool import create_agent\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "# Define available memory namespaces and their purposes\n",
    "MEMORY_NAMESPACES = {\n",
    "    \"user_memories\": \"Store and retrieve user-specific information, preferences, and personal data\",\n",
    "    \"agent_memory_survey\": \"Search the comprehensive research paper about agent memory mechanisms\",\n",
    "    \"conversations\": \"Store and retrieve conversation history and context\",\n",
    "    \"user_preferences\": \"Manage user settings, preferences, and configuration\",\n",
    "    \"project_knowledge\": \"Store and retrieve project-specific knowledge and documentation\",\n",
    "    \"research_papers\": \"General research paper storage and retrieval\",\n",
    "}\n",
    "\n",
    "def create_dynamic_memory_tool_registry() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a registry of memory tools that can be dynamically retrieved.\n",
    "    This scales much better than having all tools in an array.\n",
    "    \"\"\"\n",
    "    tool_registry = {}\n",
    "    \n",
    "    # Create search and manage tools for each namespace\n",
    "    for namespace, description in MEMORY_NAMESPACES.items():\n",
    "        namespace_tuple = (namespace,)\n",
    "        \n",
    "        # Search tool for this namespace\n",
    "        search_tool = create_search_memory_tool(namespace_tuple)\n",
    "        search_tool.description = f\"Search {description.lower()}\"\n",
    "        search_id = f\"search_{namespace}\"\n",
    "        tool_registry[search_id] = search_tool\n",
    "        \n",
    "        # Manage tool for this namespace  \n",
    "        manage_tool = create_manage_memory_tool(namespace_tuple)\n",
    "        manage_tool.description = f\"Store/update information in {description.lower()}\"\n",
    "        manage_id = f\"manage_{namespace}\"\n",
    "        tool_registry[manage_id] = manage_tool\n",
    "    \n",
    "    return tool_registry\n",
    "\n",
    "# Create the tool registry\n",
    "memory_tool_registry = create_dynamic_memory_tool_registry()\n",
    "\n",
    "print(f\"Created {len(memory_tool_registry)} memory tools:\")\n",
    "for tool_id, tool in memory_tool_registry.items():\n",
    "    print(f\"  - {tool_id}: {tool.description}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory tools indexed for semantic retrieval\n"
     ]
    }
   ],
   "source": [
    "# Index memory tools in a store for semantic retrieval\n",
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "\n",
    "# Create a separate store for tool indexing\n",
    "tool_store = MongoDBStore(\n",
    "    collection=db[\"memory_tools\"],\n",
    "    index_config=VectorIndexConfig(\n",
    "        dims=1024,\n",
    "        index_name=\"memory_tools_index\", \n",
    "        embed=VoyageAIEmbeddings(model=\"voyage-3-large\"),\n",
    "        filters=None,\n",
    "        fields=None,\n",
    "        auto_index_timeout=70\n",
    "    )\n",
    ")\n",
    "\n",
    "# Index each tool with its description for semantic search\n",
    "for tool_id, tool in memory_tool_registry.items():\n",
    "    tool_store.put(\n",
    "        (\"memory_tools\",),\n",
    "        tool_id,\n",
    "        {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"namespace\": tool_id.split(\"_\", 1)[1],  # Extract namespace from ID\n",
    "            \"operation\": tool_id.split(\"_\", 1)[0],  # Extract operation (search/manage)\n",
    "        },\n",
    "    )\n",
    "\n",
    "print(\"Memory tools indexed for semantic retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom tool retrieval function created\n"
     ]
    }
   ],
   "source": [
    "def retrieve_memory_tools(\n",
    "    query: str,\n",
    "    operation_type: Literal[\"search\", \"manage\", \"both\"] = \"both\",\n",
    "    *,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Intelligently retrieve memory tools based on query and operation type.\n",
    "    \n",
    "    Args:\n",
    "        query: The user's query to determine relevant memory namespaces\n",
    "        operation_type: Whether to retrieve search tools, manage tools, or both\n",
    "        store: The injected tool store for searching\n",
    "    \n",
    "    Returns:\n",
    "        List of tool IDs that are relevant to the query\n",
    "    \"\"\"\n",
    "    # Search for relevant tools based on the query\n",
    "    results = store.search((\"memory_tools\",), query=query, limit=6)\n",
    "    \n",
    "    # Filter by operation type if specified\n",
    "    tool_ids = []\n",
    "    for result in results:\n",
    "        tool_id = result.key\n",
    "        tool_data = result.value\n",
    "        \n",
    "        if operation_type == \"both\":\n",
    "            tool_ids.append(tool_id)\n",
    "        elif operation_type == \"search\" and tool_data[\"operation\"] == \"search\":\n",
    "            tool_ids.append(tool_id)\n",
    "        elif operation_type == \"manage\" and tool_data[\"operation\"] == \"manage\":\n",
    "            tool_ids.append(tool_id)\n",
    "    \n",
    "    # Ensure we always include the research paper search tool for research questions\n",
    "    research_indicators = [\"paper\", \"research\", \"author\", \"study\", \"survey\", \"memory mechanism\"]\n",
    "    if any(indicator in query.lower() for indicator in research_indicators):\n",
    "        if \"search_agent_memory_survey\" not in tool_ids:\n",
    "            tool_ids.insert(0, \"search_agent_memory_survey\")\n",
    "    \n",
    "    # Limit to avoid overwhelming the LLM\n",
    "    return tool_ids[:4]\n",
    "\n",
    "print(\"Custom tool retrieval function created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_memory_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
