{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -Uq oracledb sqlalchemy pandas sentence-transformers datasets einops \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cf528",
   "metadata": {},
   "source": [
    "# Part 1. Oracle AI Database Local Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20816044",
   "metadata": {},
   "source": [
    "## 1.1 Local Installation of Oracle AI Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687fd63",
   "metadata": {},
   "source": [
    "**Oracle AI Database 26ai** is a **converged database** built for AI developers.  \n",
    "It unifies **relational, document, graph and vector data** in one engine â€” so you can build  \n",
    "**semantic search**, **RAG**, and **natural language to SQL** applications without leaving the database.  \n",
    "\n",
    "Store embeddings, run vector search, and apply AI directly where your data lives â€”  \n",
    "**securely, efficiently, and at scale.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ad10f4",
   "metadata": {},
   "source": [
    "For this notebook we will be using a local installation of [Oracle AI Database](https://www.oracle.com/database/free/get-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308b61d",
   "metadata": {},
   "source": [
    "1. Install & start Docker. Docker Desktop (Mac/Windows) or Docker Engine (Linux). Make sure itâ€™s running.\n",
    "    - If installed with Docker Enginer, run from terminal ```open /Applications/Docker.app```\n",
    "2. Pull [docker image](https://www.oracle.com/database/free/get-started/)\n",
    "3. Run a container with oracle image\n",
    "\n",
    "    ```\n",
    "    docker run -d \\\n",
    "      --name oracle-full \\\n",
    "      -p 1521:1521 -p 5500:5500 \\\n",
    "      -e ORACLE_PWD=OraclePwd_2025 \\\n",
    "      -e ORACLE_SID=FREE \\\n",
    "      -e ORACLE_PDB=FREEPDB1 \\\n",
    "      -v ~/oracle/full_data:/opt/oracle/oradata \\\n",
    "      container-registry.oracle.com/database/free:latest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0da924",
   "metadata": {},
   "source": [
    "> ðŸš« **Troubleshoot**  \n",
    "> If you see the error:  \n",
    "> *`docker: Error response from daemon: Conflict. The container name \"/oracle-full\" is already in use by container ... You have to remove (or rename) that container to be able to reuse that name.`*  \n",
    ">\n",
    "> ðŸ§© **Fix:**  \n",
    "> - Remove the existing container:  \n",
    ">   ```bash\n",
    ">   docker rm oracle-full\n",
    ">   ```  \n",
    "> - Then re-run your Docker command from **Step 3** to start a new container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c21e0d",
   "metadata": {},
   "source": [
    "After running the docker command above in your terminal, you should see the image below if you click into the container running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61486cb",
   "metadata": {},
   "source": [
    "![Docker Container Log](./images/docker_container_image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c1481",
   "metadata": {},
   "source": [
    "### 1.1.1 Connecting to Oracle AI Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8defa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oracledb\n",
    "\n",
    "conn = oracledb.connect(\n",
    "    user=\"system\",\n",
    "    password=\"OraclePwd_2025\", # must match ORACLE_PWD above\n",
    "    dsn=\"localhost:1521/FREEPDB1\"\n",
    ")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT banner FROM v$version WHERE banner LIKE 'Oracle%';\")\n",
    "    print(cur.fetchone()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31988f1d",
   "metadata": {},
   "source": [
    "> ðŸš« Troubleshoot: Oracle Database Free (Docker) â€” Connection Fix\n",
    ">\n",
    "> If you see errors like:\n",
    ">\n",
    "> ```\n",
    "> OperationalError: DPY-6005: cannot connect to database\n",
    "> DPY-4011: the database or network closed the connection\n",
    "> TNS-12545: Connect failed because target host or object does not exist\n",
    "> ```\n",
    ">\n",
    "> It means the **Oracle listener** inside the container is binding to the **container hostname** instead of `0.0.0.0`, preventing host connections.\n",
    ">\n",
    ">\n",
    ">\n",
    "> ðŸ§© Fix\n",
    "> \n",
    "> Run this exact command to patch the listener and restart it:\n",
    "> \n",
    "> ```bash\n",
    "> docker exec -it oracle-full bash -lc '\n",
    ">   export ORACLE_HOME=${ORACLE_HOME:-/opt/oracle/product/26ai/dbhomeFree}\n",
    ">   export PATH=$ORACLE_HOME/bin:$PATH\n",
    ">   LISTENER_ORA=\"$ORACLE_HOME/network/admin/listener.ora\"\n",
    "> \n",
    ">   echo \"== Fixing listener to use HOST=0.0.0.0\"\n",
    ">   sed -i \"s/(HOST *= *[^)]*)/(HOST = 0.0.0.0)/\" \"$LISTENER_ORA\"\n",
    "> \n",
    ">   echo \"== Restarting listener\"\n",
    ">   lsnrctl stop || true\n",
    ">   lsnrctl start\n",
    "> \n",
    ">   echo \"== Re-registering services\"\n",
    ">   echo \"ALTER SYSTEM REGISTER;\" | sqlplus -s / as sysdba\n",
    "> \n",
    ">   echo \"== Listener status (first 20 lines):\"\n",
    ">   lsnrctl status | sed -n \"1,20p\"\n",
    "> '\n",
    "> ```\n",
    "> \n",
    "> This:\n",
    "> - Forces the listener to bind on all interfaces (`0.0.0.0`).\n",
    "> - Restarts the listener.\n",
    "> - Re-registers the PDB service (`FREEPDB1`) with the listener.\n",
    "> \n",
    "> ---\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d714bb2",
   "metadata": {},
   "source": [
    "## 1.2 Remote Access with FreeSQL.com (Coming Soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c24fa8",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02032d60",
   "metadata": {},
   "source": [
    "# Part 2. Data Loading, Preparation and Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8e19e",
   "metadata": {},
   "source": [
    "## 2.1 Data Loading From Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ddae6",
   "metadata": {},
   "source": [
    "**Streaming the ArXiv Papers Dataset with Hugging Face**\n",
    "\n",
    "The following code in the next cell demonstrates how to efficiently load and stream a large dataset using the **Hugging Face `datasets`** library â€” a powerful tool for handling massive datasets that may not fit into memory all at once.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cc928",
   "metadata": {},
   "source": [
    "**Step-by-Step Explanation**\n",
    "1. **Importing dependencies**\n",
    "   - `load_dataset` is imported from the `datasets` library, giving access to thousands of open datasets hosted on the Hugging Face Hub.\n",
    "\n",
    "2. **Loading the dataset**\n",
    "   - The dataset `\"nick007x/arxiv-papers\"` refers to a public dataset on the Hugging Face Hub that contains metadata or text from research papers hosted on [arXiv.org](https://arxiv.org).\n",
    "   - The parameter `split=\"train\"` loads the training partition of the dataset (many datasets have `train`, `validation`, and `test` splits).\n",
    "   - The key argument `streaming=True` activates **streaming mode**, meaning the dataset is read progressively from the source without downloading it entirely to disk.\n",
    "\n",
    "3. **Why streaming mode matters**\n",
    "   - Traditional dataset loading downloads the full dataset into memory or disk, which can be slow and memory-intensive.  \n",
    "   - Streaming allows you to process examples **as they arrive**, ideal for very large datasets or limited-resource environments.\n",
    "   - You can iterate over the dataset like this:\n",
    "     ```python\n",
    "     for record in ds_stream:\n",
    "         print(record)\n",
    "         break\n",
    "     ```\n",
    "\n",
    "4. **Resulting object**\n",
    "   - The variable `ds_stream` is an instance of `datasets.IterableDataset`, not a static table.  \n",
    "   - You can convert a small sample into a Pandas DataFrame for inspection:\n",
    "     ```python\n",
    "     sample = [next(iter(ds_stream)) for _ in range(5)]\n",
    "     pd.DataFrame(sample)\n",
    "     ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b06e64",
   "metadata": {},
   "source": [
    "> **ðŸ’¡ Takeaway:**  \n",
    "> Using `load_dataset(..., streaming=True)` enables developers and data scientists to work with **large datasets efficiently** â€” a perfect fit for machine learning pipelines, LLM training, or large document analysis workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_stream = load_dataset(\"nick007x/arxiv-papers\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8e1adc",
   "metadata": {},
   "source": [
    "The code below streams the first 1,000 ArXiv papers(feel free to use more) from the dataset, extracts their titles and abstracts, combines them into a single text field, and stores the results as structured dictionaries for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebcc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = []\n",
    "for i, item in enumerate(ds_stream):\n",
    "    if i >= 1000:\n",
    "        break\n",
    "    \n",
    "    arxiv_id = item.get(\"arxiv_id\", f\"unknown_{i}\")\n",
    "    title = item.get(\"title\", \"\").strip()\n",
    "    abstract = item.get(\"abstract\", \"\").strip()\n",
    "    \n",
    "    # Combine title + abstract for embedding text\n",
    "    text = f\"{title} â€” {abstract}\"\n",
    "    \n",
    "    sampled.append({\n",
    "        \"id\": item.get(\"id\", f\"arxiv_{i}\"),\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "print(f\"âœ… Streamed {len(sampled)} papers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaae87a",
   "metadata": {},
   "source": [
    "The code below converts the collected list of sampled paper records into a Pandas DataFrame for easier analysis, prints how many rows were loaded, and displays the first few entries to preview the datasetâ€™s structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b655b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tuples (id, text) into a DataFrame\n",
    "dataset_df = pd.DataFrame(sampled)\n",
    "\n",
    "# View shape and head\n",
    "print(f\"âœ… Loaded {len(dataset_df)} rows into DataFrame.\")\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac879d",
   "metadata": {},
   "source": [
    "## 2.2 Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d2c62",
   "metadata": {},
   "source": [
    "\n",
    "This code in the next cell below imports the **`SentenceTransformer`** class from the `sentence_transformers` library and loads a pretrained model called **`\"nomic-ai/nomic-embed-text-v1.5\"`** from the Hugging Face Hub.\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)\n",
    "```\n",
    "\n",
    "ðŸ” What it does\n",
    "- **`SentenceTransformer`** extends transformer-based models (like BERT or RoBERTa) to produce **sentence-level embeddings** â€” dense numerical vectors that capture the semantic meaning of text.\n",
    "- The model **`nomic-ai/nomic-embed-text-v1.5`** is optimized for general-purpose text embeddings and works well for tasks such as:\n",
    "  - Semantic search  \n",
    "  - Clustering and topic modeling  \n",
    "  - Retrieval-Augmented Generation (RAG)  \n",
    "  - Similarity ranking and recommendation systems\n",
    "- The parameter **`trust_remote_code=True`** allows the loader to execute custom code from the modelâ€™s repository, which is required for models that define specialized architectures or preprocessing logic.\n",
    "\n",
    "In short, this code prepares a powerful embedding model that can transform text (like paper titles or abstracts) into **high-dimensional semantic vectors**, making it easier to measure meaning-based similarity across documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939be3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb4f07",
   "metadata": {},
   "source": [
    "This code in the next cell below iterates through each document, encodes it into a normalized embedding vector while showing live progress, and prefixes each text with \"search_document: \" so that the Nomic embedding model correctly interprets it as a retrieval document, improving alignment with query embeddings during semantic search.\n",
    "\n",
    "The prefix `search_document`: \" tells the Nomic embedding model what kind of text itâ€™s encoding â€” in this case, a document intended for retrieval.\n",
    "\n",
    "Nomic models like nomic-embed-text-v1.5 are trained with instructional prefixes (e.g., \"search_query:\", \"search_document:\", \"classification:\"), which guide the model to generate embeddings suited for different purposes.\n",
    "\n",
    "**Why Use the `\"search_document:\"` Prefix with Nomic Embeddings**\n",
    "\n",
    "Nomic embedding models (like **`nomic-embed-text-v1.5`**) are **instruction-tuned**, meaning they were trained with specific **task prefixes** that tell the model *how* to interpret the text youâ€™re embedding.  \n",
    "\n",
    "According to the [Nomic documentation](https://docs.nomic.ai/reference/api/embed-text-v-1-embedding-text-post) and the [Hugging Face model card](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5):\n",
    "\n",
    "> â€œImportant: the text prompt must include a task instruction prefix, instructing the model which task is being performed.\n",
    "> For example, if you are implementing a RAG application, you embed your documents as  \n",
    "> `search_document: <text>` and embed your user queries as `search_query: <text>`.â€\n",
    "\n",
    "**ðŸ’¡ Why this matters**\n",
    "- The prefix tells the model whether a text is a **document** or a **query**, ensuring both are embedded into the **same semantic space**.\n",
    "- Using `search_document:` for document embeddings and `search_query:` for query embeddings **improves retrieval accuracy**, since the model optimizes for similarity between matching queryâ€“document pairs.\n",
    "- Omitting or mismatching prefixes can lead to weaker alignment and lower recall in search or RAG workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Prefix for retrieval-style embeddings\n",
    "dataset_df[\"text_prefixed\"] = dataset_df[\"text\"].apply(lambda x: f\"search_document: {x}\")\n",
    "\n",
    "# Convert to list for iteration\n",
    "texts = dataset_df[\"text_prefixed\"].tolist()\n",
    "embs = []\n",
    "\n",
    "# Encode one text at a time with a single-line progress display\n",
    "total = len(texts)\n",
    "for i, text in enumerate(texts, start=1):\n",
    "    embedding = embedding_model.encode(\n",
    "        text,\n",
    "        show_progress_bar=False,   # Disable SentenceTransformer progress\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    embs.append(embedding)\n",
    "\n",
    "    # Print progress on the same line\n",
    "    sys.stdout.write(f\"\\rEncoding {i}/{total} texts...\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Final newline to avoid overwriting the last line\n",
    "print(f\"\\nâœ… Finished encoding {len(embs)} texts.\")\n",
    "\n",
    "# Convert list of vectors to NumPy array\n",
    "embs = np.vstack(embs)\n",
    "print(f\"âœ… Embeddings shape: {embs.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2986b5f5",
   "metadata": {},
   "source": [
    "One important detail to note is the embedding dimensionality â€” the number of numerical features in each vector representation.\n",
    "This dimensionality determines the structure of your vector index and must remain consistent across all embeddings to ensure efficient similarity search and accurate retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5614e",
   "metadata": {},
   "source": [
    "This code in the next cell below converts each embedding into a list of 32-bit floating-point numbers (float32) so it can be stored in an Oracle VECTOR column, determines the embedding dimension (needed for defining the vector index), and we then displays the first two rows to confirm the data and embeddings were formatted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53954b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store as float32 lists (ready for Oracle VECTOR column)\n",
    "dataset_df[\"embedding\"] = [e.astype(np.float32).tolist() for e in embs]\n",
    "\n",
    "dim = len(dataset_df[\"embedding\"].iloc[0])\n",
    "\n",
    "# View the first 2 rows of the dataset\n",
    "dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58889766",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526f50c",
   "metadata": {},
   "source": [
    "# Part 3: Database Table Setup and Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9710e3",
   "metadata": {},
   "source": [
    "This code below drops and recreates a table called research_papers with columns for paper metadata and a VECTOR column that stores embeddings of size `dim` (the embedding dimension you computed earlier), enabling Oracle to perform vector search on those embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52cfc9",
   "metadata": {},
   "source": [
    "## 3.1 Create Reseach Papers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl = f\"\"\"\n",
    "BEGIN\n",
    "    EXECUTE IMMEDIATE 'DROP TABLE research_papers';\n",
    "EXCEPTION WHEN OTHERS THEN\n",
    "    IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "END;\n",
    "/\n",
    "CREATE TABLE research_papers (\n",
    "    arxiv_id VARCHAR2(255) PRIMARY KEY,\n",
    "    title VARCHAR2(4000),\n",
    "    abstract VARCHAR2(4000),\n",
    "    text CLOB,\n",
    "    embedding VECTOR({dim}, FLOAT32)\n",
    ")\n",
    "TABLESPACE USERS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f83f9e",
   "metadata": {},
   "source": [
    "Here, weâ€™re taking the multi-statement SQL stored in `ddl`, splitting it by `/` so each command runs separately, and executing them one by one using the database cursor.  \n",
    "After all statements finish, we call `conn.commit()` to save the changes â€” effectively creating the `RESEARCH_PAPERS` table â€” and then print a confirmation message showing the vector dimension (`dim`) used for the `embedding` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20eecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    for stmt in ddl.split(\"/\"):\n",
    "        if stmt.strip():\n",
    "            cur.execute(stmt)\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Table RESEARCH_PAPERS created with VECTOR dimension:\", dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ab118",
   "metadata": {},
   "source": [
    "## 3.2 Create Indexes (Vector and Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e7ce7",
   "metadata": {},
   "source": [
    "This code in the next cell below creates a **vector index** on the `embedding` column of the `research_papers` table to enable fast similarity search.  \n",
    "The index, named `RP_VEC_IVF`, uses Oracleâ€™s `VECTOR` indexing with **IVF (Inverted File)** organization, which partitions vectors into clusters for efficient nearest-neighbor lookup.  \n",
    "Itâ€™s configured to use **cosine distance** as the similarity metric and aims for a **target accuracy of 90%**, balancing search speed and precision.  \n",
    "Finally, `conn.commit()` saves the index creation, and a confirmation message is printed once the index is successfully built.\n",
    "\n",
    "Note: In Oracleâ€™s vector indexing syntax, **ORGANIZATION NEIGHBOR PARTITIONS** specifies that the index should be built using the IVF (Inverted File) structure â€” a partition-based nearest-neighbor organization optimized for large-scale vector search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b97672",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE VECTOR INDEX RP_VEC_IVF\n",
    "        ON research_papers(embedding)\n",
    "        ORGANIZATION NEIGHBOR PARTITIONS\n",
    "        DISTANCE COSINE\n",
    "        WITH TARGET ACCURACY 90\n",
    "        TABLESPACE USERS\n",
    "    \"\"\" )\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Vector Index RP_VEC_IVF created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106fa5b5",
   "metadata": {},
   "source": [
    "> **ðŸ’¡ Knowledge Checkpoint**\n",
    ">\n",
    "> IVF (Inverted File Index) is a vector indexing technique that speeds up similarity search by clustering vectors into groups (called partitions or  centroids).\n",
    ">\n",
    "> Instead of comparing a query vector to every vector in the dataset, IVF first identifies the most relevant clusters and only searches within those > â€” drastically reducing computation.\n",
    "> \n",
    "> In Oracle AI Database, the clause ORGANIZATION NEIGHBOR PARTITIONS activates this IVF-style structure, allowing the database to perform approximate nearest-neighbor (ANN) searches efficiently while maintaining high accuracy (controlled by the WITH TARGET ACCURACY parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231597e",
   "metadata": {},
   "source": [
    "**Creating an Oracle Text Index for Full-Text Search**\n",
    "\n",
    "This code below creates a **full-text search index** on the `text` column of the `research_papers` table using Oracle Text.\n",
    "\n",
    "1. **Drop existing index** â€” removes `rp_text_idx` if it already exists, ignoring the â€œindex does not existâ€ error.  \n",
    "2. **Create new text index** â€” builds a `CTXSYS.CONTEXT` index, which tokenizes and indexes the text for efficient keyword searches.  \n",
    "   - `SYNC (ON COMMIT)` keeps the index automatically updated whenever new data is committed.  \n",
    "3. **Commit and confirm** â€” saves the changes and prints a success message.\n",
    "\n",
    "Once created, you can use the `CONTAINS()` operator with `SCORE()` for fast, ranked keyword searches, e.g.:\n",
    "\n",
    "```sql\n",
    "SELECT title, SCORE(1) AS relevance\n",
    "FROM research_papers\n",
    "WHERE CONTAINS(text, 'transformer architecture', 1) > 0\n",
    "ORDER BY SCORE(1) DESC;\n",
    "```\n",
    "\n",
    "This turns your text column into a search-optimized field, similar to how search engines handle full-text retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52720503",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    # Drop old index if it exists\n",
    "    try:\n",
    "        cur.execute(\"DROP INDEX rp_text_idx\")\n",
    "    except oracledb.DatabaseError as e:\n",
    "        if \"ORA-01418\" not in str(e):  # ignore \"index does not exist\"\n",
    "            raise\n",
    "\n",
    "    # Create a TEXT index on the 'text' column only\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX rp_text_idx\n",
    "        ON research_papers(text)\n",
    "        INDEXTYPE IS CTXSYS.CONTEXT\n",
    "        PARAMETERS ('SYNC (ON COMMIT)')\n",
    "    \"\"\")\n",
    "\n",
    "conn.commit()\n",
    "print(\"âœ… Oracle Text index (rp_text_idx) created successfully on TEXT column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d74db",
   "metadata": {},
   "source": [
    "In the code below weâ€™re taking all the research paper records (including their embeddings) from the DataFrame and inserting them into the `RESEARCH_PAPERS` table in Oracle.\n",
    "\n",
    "Letâ€™s break down whatâ€™s happening:\n",
    "\n",
    "\n",
    "1. Convert each embedding for Oracle compatibility\n",
    "```python\n",
    "embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "```\n",
    "Oracleâ€™s `VECTOR` column expects the data as a compact binary float array, not a Python list.  \n",
    "So here we convert each embedding into an `array.array('f')` â€” this ensures the data binds correctly and efficiently when inserting.\n",
    "\n",
    "\n",
    "2. Prepare all rows for insertion\n",
    "For every paper, we build a tuple containing its metadata (`arxiv_id`, `title`, `abstract`, `text`) and the formatted `embedding_array`.  \n",
    "These tuples are collected in a list called `rows`.\n",
    "\n",
    "3. Insert each row with a progress bar\n",
    "```python\n",
    "for row in tqdm(rows):\n",
    "    cur.execute(\"\"\"\n",
    "        INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "        VALUES (:1, :2, :3, :4, :5)\n",
    "    \"\"\", row)\n",
    "```\n",
    "We loop through each row, inserting it into the table using Oracleâ€™s parameterized query syntax.  \n",
    "The `tqdm` progress bar gives real-time feedback on the insertion process â€” helpful when inserting hundreds or thousands of embeddings.\n",
    "\n",
    "\n",
    "4. Commit everything\n",
    "Finally, `conn.commit()` saves all the inserted records permanently in the database.  Youâ€™ll see a confirmation message once the operation completes successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08324270",
   "metadata": {},
   "source": [
    "## 3.3 Ingest Data into Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bdde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import array\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, row in dataset_df.iterrows():\n",
    "    # Convert embedding list to array.array for proper VECTOR binding\n",
    "    embedding_array = array.array('f', row.get(\"embedding\"))\n",
    "    \n",
    "    rows.append((\n",
    "        row.get(\"arxiv_id\"),\n",
    "        row.get(\"title\"),\n",
    "        row.get(\"abstract\"),\n",
    "        row.get(\"text\"),\n",
    "        embedding_array\n",
    "    ))\n",
    "\n",
    "print(f\"Preparing to insert {len(rows)} rows into RESEARCH_PAPERS...\")\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    for row in tqdm(rows):\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO research_papers (arxiv_id, title, abstract, text, embedding)\n",
    "            VALUES (:1, :2, :3, :4, :5)\n",
    "            \"\"\", \n",
    "            row\n",
    "        )\n",
    "        \n",
    "conn.commit()\n",
    "print(\"âœ… Data inserted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a14d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT COUNT(*) FROM RESEARCH_PAPERS\")\n",
    "    print(\"Row count:\", cur.fetchone()[0])\n",
    "\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT arxiv_id, title, abstract, text FROM RESEARCH_PAPERS\n",
    "        FETCH FIRST 3 ROWS ONLY\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9788fc",
   "metadata": {},
   "source": [
    "# Part 4. Retrieval Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_TEXT_KEYWORDS = \"optimization\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03c53c",
   "metadata": {},
   "source": [
    "## 4.1 Text Based Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d31cd",
   "metadata": {},
   "source": [
    "The code below performs a full-text search over the text column of the research_papers table, using the Oracle Text index we just created earlier\n",
    "\n",
    "- CONTAINS(text, :keyword, 1) uses the Oracle Text index on the text column to find documents containing the given keyword or phrase.\n",
    "- SCORE(1) assigns a relevance score based on how well each document matches the search term.\n",
    "- SUBSTR(text, 1, 200) returns the first 200 characters as a short preview snippet.\n",
    "- FETCH FIRST 10 ROWS ONLY limits the results to the top 10 most relevant matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fbb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search_research_papers(conn, keyword: str):\n",
    "    \"\"\"\n",
    "    Perform a full-text keyword search on the 'text' column \n",
    "    using the Oracle Text index (rp_text_idx).\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        keyword (str): Keyword or phrase to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns)\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            SCORE(1) AS relevance_score\n",
    "        FROM research_papers\n",
    "        WHERE CONTAINS(text, :keyword, 1) > 0\n",
    "        ORDER BY SCORE(1) DESC\n",
    "        FETCH FIRST 10 ROWS ONLY\n",
    "    \"\"\"\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, keyword=keyword)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = keyword_search_research_papers(conn, SEARCH_TEXT_KEYWORDS)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"ðŸ” Keyword Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"ðŸ“Š Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c352afa",
   "metadata": {},
   "source": [
    "As shown above, the returned rows represent documents whose indexed `text` content matched the full-text search query **\"Optimization\"**, as determined by the Oracle Text `CONTAINS()` operator using the `rp_text_idx` index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff4f6b",
   "metadata": {},
   "source": [
    "## 4.2 Vector Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERY = \"Get me papers related to planetary exploration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c96d4",
   "metadata": {},
   "source": [
    "**Vector Similarity Search in Oracle**\n",
    "\n",
    "This function below `vector_search_research_papers` performs a **semantic vector search** on the `research_papers` table, retrieving papers most similar to a given query using **Oracleâ€™s native VECTOR search** feature.\n",
    "\n",
    "\n",
    "Step-by-step overview\n",
    "\n",
    "1. Encode the search query\n",
    "```python\n",
    "query_embedding = embedding_model.encode(\n",
    "    [f\"search_query: {search_query}\"],\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")[0].astype(np.float32).tolist()\n",
    "```\n",
    "- The query is embedded using the same model that generated the document embeddings.  \n",
    "- The prefix `\"search_query:\"` ensures embeddings align with `\"search_document:\"` vectors.  \n",
    "- Normalized and cast to `float32` for Oracleâ€™s `VECTOR` type.\n",
    "\n",
    "\n",
    "2. Prepare for database binding\n",
    "```python\n",
    "query_embedding_array = array.array('f', query_embedding)\n",
    "```\n",
    "- Converts the embedding list into a binary float array (`array('f')`), required for Oracleâ€™s vector binding.\n",
    "\n",
    "\n",
    "3. Perform vector search\n",
    "```sql\n",
    "SELECT arxiv_id, title, abstract,\n",
    "       SUBSTR(text, 1, 200) AS text_snippet,\n",
    "        ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "FROM research_papers\n",
    "ORDER BY similarity_score\n",
    "FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "```\n",
    "- Computes **cosine distance** between the query vector and stored embeddings.  \n",
    "- Orders results by similarity, returning the closest matches.  \n",
    "- Uses **Approximate Nearest Neighbor (ANN)** search for fast retrieval at 90% target accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_research_papers(conn, embedding_model, search_query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a vector similarity search on the research_papers table using a query embedding.\n",
    "    Returns cosine similarity scores (higher = more similar).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1ï¸âƒ£ Encode the query into a vector\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_query}\"], \n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "\n",
    "    # 2ï¸âƒ£ Prepare the vector for Oracle binding\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    # 3ï¸âƒ£ Run a vector similarity search using cosine similarity\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            arxiv_id, \n",
    "            title, \n",
    "            abstract, \n",
    "            SUBSTR(text, 1, 200) AS text_snippet,\n",
    "            ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "        FROM research_papers\n",
    "        ORDER BY similarity_score DESC\n",
    "        FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "    \"\"\"\n",
    "\n",
    "    # 4ï¸âƒ£ Execute and return results\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(query, q=query_embedding_array)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    return rows, columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca16bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = vector_search_research_papers(conn, embedding_model, SEARCH_QUERY, top_k=5)\n",
    "\n",
    "results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"ðŸ” Vector Search: '{SEARCH_QUERY}'\")\n",
    "print(f\"ðŸ“Š Results: {len(results_df)}\\n\")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8bf3e",
   "metadata": {},
   "source": [
    "In this step, we used Oracle Databaseâ€™s native vector search capabilities to perform a semantic similarity search over the research_papers dataset.\n",
    "\n",
    "The text query â€” â€œGet me papers related to planetary explorationâ€ â€” was first transformed into a high-dimensional embedding vector using the same model that generated our document embeddings (nomic-embed-text-v1.5).\n",
    "This query vector captures the semantic meaning of the phrase rather than just the exact words.\n",
    "\n",
    "Using Oracleâ€™s VECTOR_DISTANCE(..., COSINE) function (converted to 1 - distance), we then compared this query vector against the stored embeddings for each paper in the database.\n",
    "The result is a ranked list of research papers ordered by cosine similarity, where higher scores indicate stronger semantic alignment with the search query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fd0c7",
   "metadata": {},
   "source": [
    "## 4.3 Hybrid Retrieval (Vector + Text Combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1972f31",
   "metadata": {},
   "source": [
    "#### 4.3.1 Pre Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Oracle Text + Vector Search.\n",
    "    Combines lexical filtering (CONTAINS) with semantic re-ranking via cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): User search phrase used for both text filtering and embedding.\n",
    "        top_k (int): Number of results to return (default = 10).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into normalized vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime stats if needed\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Oracle Text + Vector) ---\n",
    "        sql = f\"\"\"\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                abstract,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(1 - VECTOR_DISTANCE(embedding, :q, COSINE), 4) AS similarity_score\n",
    "            FROM research_papers\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH APPROX FIRST {top_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba38db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_pre_filter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "pre_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"ðŸ” Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"ðŸ“Š Found {len(pre_filter_results_df)} results\\n\")\n",
    "\n",
    "pre_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5fda2",
   "metadata": {},
   "source": [
    "#### 4.3.2 Post Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae572091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    candidate_k: int = 200,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a hybrid search using Vector Search first, then Oracle Text filtering.\n",
    "    Returns top results ranked by semantic similarity but filtered by lexical match.\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection object.\n",
    "        embedding_model: Model with `.encode()` method (e.g., SentenceTransformer).\n",
    "        search_phrase (str): Search phrase used for both embedding and text filtering.\n",
    "        top_k (int): Number of top results to return (default = 10).\n",
    "        candidate_k (int): Number of initial vector candidates (default = 200).\n",
    "        show_explain (bool): If True, prints the execution plan.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (rows, columns, exec_plan_text or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Encode search phrase into a normalized query vector ---\n",
    "    query_embedding = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    query_embedding_array = array.array('f', query_embedding)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        # Enable runtime statistics if requested\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        # --- Step 2: Hybrid query (Vector first â†’ Text filter) ---\n",
    "        sql = f\"\"\"\n",
    "            WITH vec_candidates AS (\n",
    "                SELECT\n",
    "                    arxiv_id,\n",
    "                    title,\n",
    "                    abstract,\n",
    "                    text,\n",
    "                    1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS similarity_score\n",
    "                FROM research_papers\n",
    "                ORDER BY similarity_score DESC\n",
    "                FETCH APPROX FIRST {candidate_k} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                ROUND(similarity_score, 4) AS similarity_score\n",
    "            FROM vec_candidates\n",
    "            WHERE CONTAINS(text, :kw, 1) > 0\n",
    "            ORDER BY similarity_score DESC\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=query_embedding_array, kw=search_phrase)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "\n",
    "    # --- Step 3: Fetch and display execution plan (optional) ---\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "\n",
    "        print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4048cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns, exec_plan = hybrid_search_research_papers_postfilter(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "post_filter_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"ðŸ” Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"ðŸ“Š Found {len(post_filter_results_df)} results\\n\")\n",
    "\n",
    "post_filter_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba61b5",
   "metadata": {},
   "source": [
    "Observe pre/post filtering technques in a table side by side "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e521f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results side by side\n",
    "pd.concat([pre_filter_results_df, post_filter_results_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041cd8b",
   "metadata": {},
   "source": [
    "| Approach                          | Strength                            | Best For                                |\n",
    "| --------------------------------- | ----------------------------------- | --------------------------------------- |\n",
    "| **Pre-filter** (`CONTAINS` first) | Fast, keyword-strict                | Narrow keyword search                   |\n",
    "| **Post-filter** (this one)        | Semantically rich but still precise | Broader exploratory or research queries |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a008720",
   "metadata": {},
   "source": [
    "#### 4.3.3 Reciprocial Rank Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import numpy as np\n",
    "import oracledb\n",
    "\n",
    "def hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase: str,\n",
    "    top_k: int = 10,\n",
    "    per_list: int = 120,     # candidates from each list before fusion (>= 10x top_k is a good rule)\n",
    "    k: int = 60,             # RRF smoothing constant (60 is standard)\n",
    "    phrase_safe: bool = True,\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Local-friendly RRF fusion of Vector + Oracle Text results on research_papers(text, embedding).\n",
    "\n",
    "    Prereqs (local/Docker Free OK):\n",
    "      - VECTOR column/index on research_papers(embedding)  -- IVF/HNSW\n",
    "      - Oracle Text index on research_papers(text)         -- e.g. CREATE SEARCH INDEX rp_text_idx ON research_papers(text);\n",
    "\n",
    "    RRF = 1/(k + r_vec) + 1/(k + r_txt), where r_vec and r_txt are ranks (1 = best).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Encode query for vector modality (align with your doc prefixing scheme)\n",
    "    qv = embedding_model.encode(\n",
    "        [f\"search_query: {search_phrase}\"],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )[0].astype(np.float32).tolist()\n",
    "    qv = array.array('f', qv)\n",
    "\n",
    "    # 2) Phrase-safe text query for Oracle Text (optional)\n",
    "    kw = f\"\\\"{search_phrase}\\\"\" if (phrase_safe and \" \" in search_phrase.strip()) else search_phrase\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        if show_explain:\n",
    "            cur.execute(\"ALTER SESSION SET statistics_level = ALL\")\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            WITH\n",
    "            /* Vector top-N with ranks (higher similarity first) */\n",
    "            vec AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                1 - VECTOR_DISTANCE(embedding, :q, COSINE) AS sim_vec,\n",
    "                ROW_NUMBER() OVER (ORDER BY 1 - VECTOR_DISTANCE(embedding, :q, COSINE) DESC) AS r_vec\n",
    "              FROM research_papers\n",
    "              FETCH APPROX FIRST {per_list} ROWS ONLY WITH TARGET ACCURACY 90\n",
    "            ),\n",
    "            /* Oracle Text top-N with ranks (higher SCORE(1) first) */\n",
    "            txt AS (\n",
    "              SELECT\n",
    "                arxiv_id,\n",
    "                title,\n",
    "                SUBSTR(text, 1, 200) AS text_snippet,\n",
    "                SCORE(1) AS score_txt,\n",
    "                ROW_NUMBER() OVER (ORDER BY SCORE(1) DESC) AS r_txt\n",
    "              FROM research_papers\n",
    "              WHERE CONTAINS(text, :kw, 1) > 0\n",
    "              FETCH FIRST {per_list} ROWS ONLY\n",
    "            ),\n",
    "            /* Fuse by arxiv_id; keep docs present in either list */\n",
    "            fused AS (\n",
    "              SELECT\n",
    "                COALESCE(v.arxiv_id, t.arxiv_id)           AS arxiv_id,\n",
    "                COALESCE(v.title,     t.title)             AS title,\n",
    "                COALESCE(v.text_snippet, t.text_snippet)   AS text_snippet,\n",
    "                NVL(v.r_vec,  999999) AS r_vec,\n",
    "                NVL(t.r_txt,  999999) AS r_txt,\n",
    "                NVL(v.sim_vec, 0)     AS sim_vec,\n",
    "                NVL(t.score_txt, 0)   AS score_txt\n",
    "              FROM vec v\n",
    "              FULL OUTER JOIN txt t\n",
    "                ON t.arxiv_id = v.arxiv_id\n",
    "            )\n",
    "            SELECT {\"/*+ GATHER_PLAN_STATISTICS */\" if show_explain else \"\"}\n",
    "              arxiv_id,\n",
    "              title,\n",
    "              text_snippet,\n",
    "              ROUND( (1.0/(:k + r_vec)) + (1.0/(:k + r_txt)), 6 ) AS rrf_score,\n",
    "              r_vec,\n",
    "              r_txt,\n",
    "              ROUND(sim_vec, 4)  AS sim_vec,\n",
    "              ROUND(score_txt,4) AS score_txt\n",
    "            FROM fused\n",
    "            ORDER BY rrf_score DESC, title\n",
    "            FETCH FIRST {top_k} ROWS ONLY\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(sql, q=qv, kw=kw, k=k)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [d[0] for d in cur.description]\n",
    "\n",
    "    exec_plan_text = None\n",
    "    if show_explain:\n",
    "        with conn.cursor() as cur_plan:\n",
    "            cur_plan.execute(\"\"\"\n",
    "                SELECT plan_table_output\n",
    "                FROM TABLE(DBMS_XPLAN.DISPLAY_CURSOR(NULL, NULL, 'ALLSTATS LAST +PREDICATE'))\n",
    "            \"\"\")\n",
    "            exec_plan_text = \"\\n\".join(r[0] for r in cur_plan.fetchall())\n",
    "            print(\"\\n====== Execution Plan (DBMS_XPLAN.DISPLAY_CURSOR) ======\")\n",
    "            print(exec_plan_text)\n",
    "            print(\"========================================================\\n\")\n",
    "\n",
    "    return rows, columns, exec_plan_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns, exec_plan = hybrid_rrf_search(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    search_phrase=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=10,\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "rrf_results_df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "print(f\"ðŸ” Hybrid Search: '{SEARCH_TEXT_KEYWORDS}'\")\n",
    "print(f\"ðŸ“Š Found {len(rrf_results_df)} results\\n\")\n",
    "\n",
    "rrf_results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0832de",
   "metadata": {},
   "source": [
    "# Part 5: Building a RAG pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Function to securely get and set environment variables\n",
    "def set_env_securely(var_name, prompt):\n",
    "    value = getpass.getpass(prompt)\n",
    "    os.environ[var_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_env_securely(\"OPENAI_API_KEY\", \"Enter your OPEN API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI Python client library\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the OpenAI client (API key read from env var OPENAI_API_KEY)\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Create a Response API request (using the Responses API)\n",
    "\n",
    "# Use the Responses API\n",
    "response = openai_client.responses.create(\n",
    "    model=\"gpt-4o\",              # specify model\n",
    "    input=\"Hello! Iâ€™m a user!\",  # user message as a text input\n",
    "    instructions=\"You are a research paper assistant.\",  # assistant role/instruction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27805348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output text\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_paper_assistant_rag_pipeline(\n",
    "    conn,\n",
    "    embedding_model,\n",
    "    user_query: str,\n",
    "    top_k: int = 10,\n",
    "    retrieval_mode: str = \"hybrid\",\n",
    "    show_explain: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Research Paper Assistant â€” Retrieval-Augmented Generation (RAG) pipeline\n",
    "    built on SQL-based retrieval functions and powered by the OpenAI Responses API.\n",
    "\n",
    "    Retrieval techniques available:\n",
    "        - 'keyword'  â†’ uses keyword_search_research_papers()\n",
    "        - 'vector'   â†’ uses vector_search_research_papers()\n",
    "        - 'hybrid'   â†’ uses hybrid_search_research_papers() [default]\n",
    "\n",
    "    Args:\n",
    "        conn: Oracle database connection.\n",
    "        embedding_model: Embedding model (e.g., SentenceTransformer, Voyage).\n",
    "        user_query (str): Research question from the user.\n",
    "        top_k (int): Number of top documents to retrieve.\n",
    "        retrieval_mode (str): Retrieval method ('keyword', 'vector', 'hybrid').\n",
    "        show_explain (bool): Whether to show the SQL execution plan.\n",
    "\n",
    "    Returns:\n",
    "        str: LLM-generated research synthesis with citations.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Retrieve relevant research papers using the selected retrieval mode\n",
    "    # ----------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "        exec_plan_text = None\n",
    "\n",
    "    else:  # default: hybrid retrieval\n",
    "        rows, columns, exec_plan_text = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=show_explain\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "    print(f\"ðŸ“Š Retrieved {retrieved_count} papers using {retrieval_mode.upper()} retrieval.\")\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Convert retrieved rows to formatted LLM context\n",
    "    # ----------------------------------------------------------------------\n",
    "    formatted_context = \"\"\n",
    "    if retrieved_count > 0:\n",
    "        formatted_context += f\"\\n\\nðŸ“š {retrieved_count} relevant research papers retrieved:\\n\\n\"\n",
    "        for i, row in enumerate(rows):\n",
    "            row_data = dict(zip(columns, row))\n",
    "            title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "            abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "            snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "            score = (\n",
    "                row_data.get(\"SIMILARITY_SCORE\")\n",
    "                or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "                or \"N/A\"\n",
    "            )\n",
    "            formatted_context += (\n",
    "                f\"[{i+1}] **{title}**\\n\"\n",
    "                f\"Abstract: {abstract}\\n\"\n",
    "                f\"Snippet: {snippet}\\n\"\n",
    "                f\"Relevance Score: {score}\\n\\n\"\n",
    "            )\n",
    "    else:\n",
    "        formatted_context = \"\\n\\nâš ï¸ No relevant papers were retrieved from the database.\\n\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 3. Construct the prompt for the Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    prompt = f\"\"\"\n",
    "            You are a **Research Paper Assistant** that synthesizes academic literature to help answer user questions.\n",
    "\n",
    "            User Query: {user_query}\n",
    "\n",
    "            Number of retrieved papers: {retrieved_count}\n",
    "            {formatted_context}\n",
    "\n",
    "            Please:\n",
    "            - Summarize the findings most relevant to the query.\n",
    "            - Use citation numbers [X] to support claims.\n",
    "            - Highlight consensus, innovation, or research gaps.\n",
    "            - If there is insufficient context, clearly say so.\n",
    "            \"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 4. Call the OpenAI Responses API\n",
    "    # ----------------------------------------------------------------------\n",
    "    response = openai_client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        input=prompt,\n",
    "        instructions=\"You are a scientific research assistant. Use only the provided context to answer. Always cite papers [1], [2], etc.\",\n",
    "        temperature=0.3,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 5. Optionally print SQL execution plan (if hybrid)\n",
    "    # ----------------------------------------------------------------------\n",
    "    if show_explain and exec_plan_text:\n",
    "        print(\"\\n====== SQL Execution Plan ======\")\n",
    "        print(exec_plan_text)\n",
    "        print(\"================================\\n\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 6. Return the LLMâ€™s output text\n",
    "    # ----------------------------------------------------------------------\n",
    "    return response.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = research_paper_assistant_rag_pipeline(\n",
    "    conn=conn,\n",
    "    embedding_model=embedding_model,\n",
    "    user_query=SEARCH_TEXT_KEYWORDS,\n",
    "    top_k=5,\n",
    "    retrieval_mode=\"hybrid\",  # options: 'keyword', 'vector', 'hybrid'\n",
    "    show_explain=False\n",
    ")\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104b31b8",
   "metadata": {},
   "source": [
    "# Part 6: AI Agents with OpenAI and Oracle AI Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74defe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "      You are a Research Paper Assistant focused on helping users explore, analyze, and summarize\n",
    "      academic research.\n",
    "\n",
    "      Maintain a professional, concise, and scholarly tone appropriate for research discussions.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ab0778",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Summarize recent research on optimization techniques for planetary exploration.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eda5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_result.final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_research_papers(user_query: str, retrieval_mode: str = \"hybrid\", top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves academic research papers relevant to the user's query.\n",
    "\n",
    "    This tool queries the research_papers SQL table using one of three retrieval techniques:\n",
    "        - 'keyword'  â†’ lexical search via LIKE filtering\n",
    "        - 'vector'   â†’ semantic similarity search\n",
    "        - 'hybrid'   â†’ combines keyword prefiltering + vector similarity (default)\n",
    "\n",
    "    Use this tool when analyzing or summarizing scientific literature.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): Research topic or question to search for.\n",
    "        retrieval_mode (str): 'keyword', 'vector', or 'hybrid'. Default is 'hybrid'.\n",
    "        top_k (int): Number of top papers to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted summary of the most relevant research papers.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using SQL-based functions (defined earlier)\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieval_mode == \"keyword\":\n",
    "        rows, columns = keyword_search_research_papers(conn, user_query)\n",
    "    elif retrieval_mode == \"vector\":\n",
    "        rows, columns = vector_search_research_papers(conn, embedding_model, user_query, top_k)\n",
    "    else:\n",
    "        rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "            conn=conn,\n",
    "            embedding_model=embedding_model,\n",
    "            search_phrase=user_query,\n",
    "            top_k=top_k,\n",
    "            show_explain=False\n",
    "        )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format the output into a readable string\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No research papers found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"ðŸ“š {retrieved_count} papers retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Paper\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No abstract available.\")\n",
    "        score = (\n",
    "            row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] {title}\\n\"\n",
    "            f\"Abstract: {abstract}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_paper_assistant.tools.append(get_research_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdca1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tool = await Runner.run(\n",
    "    starting_agent=research_paper_assistant,\n",
    "    input=\"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92279dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_result_with_tool.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(run_result_with_tool.raw_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a451b",
   "metadata": {},
   "source": [
    "### Build an Agent with Multiple Tool Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c0266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.tool import function_tool\n",
    "\n",
    "@function_tool\n",
    "def get_past_research_conversations(user_query: str, top_k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves relevant past research-related conversations or analyses related to the query.\n",
    "\n",
    "    This tool searches a SQL database of prior research assistant conversations, \n",
    "    literature discussions, or synthesis sessions to find relevant context. \n",
    "    It allows the research assistant to recall previous analyses or summaries \n",
    "    that addressed similar topics, providing continuity and richer insights.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The research topic, concept, or question to search for.\n",
    "        top_k (int): Number of top past discussions to retrieve (default=5).\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted examples of relevant past research discussions.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Perform retrieval using the SQL-based hybrid search (vector + keyword)\n",
    "    # ------------------------------------------------------------------\n",
    "    rows, columns, _ = hybrid_search_research_papers_pre_filter(\n",
    "        conn=conn,\n",
    "        embedding_model=embedding_model,\n",
    "        search_phrase=user_query,\n",
    "        top_k=top_k,\n",
    "        show_explain=False\n",
    "    )\n",
    "\n",
    "    retrieved_count = len(rows) if rows else 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Format results for readability\n",
    "    # ------------------------------------------------------------------\n",
    "    if retrieved_count == 0:\n",
    "        return f\"No past research discussions found related to '{user_query}'.\"\n",
    "\n",
    "    formatted_results = [f\"ðŸ§  {retrieved_count} past research discussions retrieved for query: '{user_query}'\\n\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_data = dict(zip(columns, row))\n",
    "        title = row_data.get(\"TITLE\", \"Untitled Discussion\")\n",
    "        abstract = row_data.get(\"ABSTRACT\", \"No summary available.\")\n",
    "        snippet = row_data.get(\"TEXT_SNIPPET\", \"\")\n",
    "        score = (\n",
    "            row_data.get(\"SIMILARITY_SCORE\")\n",
    "            or row_data.get(\"TEXT_RELEVANCE_SCORE\")\n",
    "            or \"N/A\"\n",
    "        )\n",
    "        formatted_results.append(\n",
    "            f\"[{i+1}] **{title}**\\n\"\n",
    "            f\"Summary: {abstract}\\n\"\n",
    "            f\"Snippet: {snippet}\\n\"\n",
    "            f\"Relevance Score: {score}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(formatted_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db0e96",
   "metadata": {},
   "source": [
    "Let's update our agent instruction to ensure it knows when to utilize the right tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f602932",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgraded_research_paper_assistant = Agent(\n",
    "    name=\"Research Paper Assistant\",\n",
    "    model=OPENAI_MODEL,\n",
    "    instructions=\"\"\"\n",
    "    Always maintain an academic, evidence-based tone.\n",
    "    Your purpose is to help users explore, synthesize, and connect research insights â€”\n",
    "    not to speculate or fabricate information.\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach research retrieval tools to the upgraded research assistant\n",
    "upgraded_research_paper_assistant.tools.append(get_research_papers)\n",
    "upgraded_research_paper_assistant.tools.append(get_past_research_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66d1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "upgraded_research_paper_assistant.tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result_with_tools = await Runner.run(\n",
    "    starting_agent=upgraded_research_paper_assistant,\n",
    "    input=(\n",
    "        \"Get me information on rover navigation, planetary data collection, mission planning, resource allocation, or other related fields \"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_result_with_tools.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a57b24",
   "metadata": {},
   "source": [
    "## Agent as Tools (Ochestration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef7841",
   "metadata": {},
   "source": [
    "Add an image of the flow of these agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specialized agents for different research retrieval tasks\n",
    "research_paper_agent = Agent(\n",
    "    name=\"research_paper_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing academic research papers.\n",
    "        Use the get_research_papers tool to find relevant literature based on the user's query.\n",
    "        Always cite sources using [1], [2], etc., and focus on summarizing key findings,\n",
    "        methodologies, and implications of the studies retrieved.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research retrieval specialist with access to academic papers and literature databases.\",\n",
    "    tools=[get_research_papers],\n",
    ")\n",
    "\n",
    "research_conversation_agent = Agent(\n",
    "    name=\"research_conversation_agent\",\n",
    "    instructions=\"\"\"\n",
    "        You specialize in retrieving and summarizing past research discussions and analyses.\n",
    "        Use the get_past_research_conversations tool to surface relevant prior sessions\n",
    "        or summaries that relate to the user's current topic of inquiry.\n",
    "        Present these as context and examples of prior analytical reasoning.\n",
    "    \"\"\",\n",
    "    handoff_description=\"A research memory specialist with access to prior academic discussions and analyses.\",\n",
    "    tools=[get_past_research_conversations],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304cac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an orchestrator agent that can coordinate both research retrieval agents\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"research_assistant_orchestrator\",\n",
    "    instructions=(\n",
    "        \"You are a Research Orchestrator Assistant responsible for coordinating information retrieval \"\n",
    "        \"across multiple specialized research tools.\\n\\n\"\n",
    "        \"Your role is to help users explore, analyze, and synthesize academic research efficiently.\\n\\n\"\n",
    "        \"IMPORTANT RULES:\\n\"\n",
    "        \"1. ALWAYS use translate_to_research_papers when a query mentions research papers, studies, or findings.\\n\"\n",
    "        \"2. ALWAYS use translate_to_research_conversations when a query mentions previous discussions, analyses, or summaries.\\n\"\n",
    "        \"3. If a query requests BOTH new research and past discussions, use BOTH tools in sequence.\\n\"\n",
    "        \"4. NEVER attempt to provide research summaries without using your tools.\\n\"\n",
    "        \"5. Each tool provides complementary context â€” use all appropriate tools for a comprehensive academic response.\\n\\n\"\n",
    "        \"After retrieving relevant results, synthesize them into a cohesive summary:\\n\"\n",
    "        \"- Clearly distinguish between newly retrieved research and recalled past discussions.\\n\"\n",
    "        \"- Cite sources using [1], [2], etc.\\n\"\n",
    "        \"- Identify key insights, trends, and research gaps.\\n\"\n",
    "        \"- Maintain an academic and objective tone.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        research_paper_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_papers\",\n",
    "            tool_description=\"Retrieve and summarize relevant academic research papers and literature findings.\",\n",
    "        ),\n",
    "        research_conversation_agent.as_tool(\n",
    "            tool_name=\"translate_to_research_conversations\",\n",
    "            tool_description=\"Retrieve and summarize past research discussions or analyses related to the topic.\",\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aab9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final agent to synthesize information from all sources (Research use case)\n",
    "synthesizer_agent = Agent(\n",
    "    name=\"research_response_synthesizer\",\n",
    "    instructions=(\n",
    "        \"You create comprehensive, well-organized research summaries by combining information from multiple sources.\\n\\n\"\n",
    "        \"When organizing your response:\\n\"\n",
    "        \"1) Start with a concise abstract-style overview (3â€“5 sentences) highlighting key findings and takeaways.\\n\"\n",
    "        \"2) Clearly separate NEW LITERATURE FINDINGS from PAST RESEARCH DISCUSSIONS.\\n\"\n",
    "        \"3) Cite sources using bracketed numbers [1], [2], etc., aligned with the retrieved items.\\n\"\n",
    "        \"4) Emphasize methods, evidence strength, and limitations; avoid speculation beyond the provided context.\\n\"\n",
    "        \"5) Use clear, scannable formatting (short paragraphs, bullet points where appropriate).\\n\"\n",
    "        \"6) Conclude with open questions, gaps, or future work suggested by the literature.\\n\"\n",
    "        \"7) If evidence is sparse, state this explicitly and avoid overgeneralization.\\n\"\n",
    "        \"Tone: academic, objective, and precise.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ItemHelpers, MessageOutputItem, trace\n",
    "from agents import Runner  # assuming Runner is imported elsewhere; include here for clarity\n",
    "\n",
    "\n",
    "async def research_assistant_workflow(user_query: str):\n",
    "    \"\"\"Run the complete research assistant workflow (orchestrate retrieval + synthesize).\"\"\"\n",
    "    # 1) Have the research orchestrator decide which tools to invoke\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, user_query)\n",
    "\n",
    "        # Debug/transparency: print intermediate orchestration steps\n",
    "        print(\"\\n--- Research Orchestration Steps ---\")\n",
    "        for item in orchestrator_result.new_items:\n",
    "            if isinstance(item, MessageOutputItem):\n",
    "                text = ItemHelpers.text_message_output(item)\n",
    "                if text:\n",
    "                    print(f\"  - Retrieval step: {text}\")\n",
    "\n",
    "        # 2) Synthesize all gathered information into a cohesive research summary\n",
    "        synthesizer_result = await Runner.run(\n",
    "            synthesizer_agent, orchestrator_result.to_input_list()\n",
    "        )\n",
    "\n",
    "        print(f\"\\n\\n--- Final Research Synthesis ---\\n{synthesizer_result.final_output}\\n\")\n",
    "\n",
    "    return synthesizer_result.final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc193e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to patch the event loop\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc86489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_virtual_research_assistant(query):\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "    # Run the async function and get the result\n",
    "    result = loop.run_until_complete(research_assistant_workflow(query))\n",
    "\n",
    "    # Clean up\n",
    "    loop.close()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the function this way\n",
    "query = input(\"What research topic can I help you with today? \")\n",
    "run_virtual_research_assistant(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e6c26",
   "metadata": {},
   "source": [
    "## Agentic Chat System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05741d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "# Create chat_history table in Oracle\n",
    "with conn.cursor() as cur:\n",
    "    # Drop table if exists (for development)\n",
    "    cur.execute(\"\"\"\n",
    "        BEGIN\n",
    "            EXECUTE IMMEDIATE 'DROP TABLE chat_history';\n",
    "        EXCEPTION WHEN OTHERS THEN\n",
    "            IF SQLCODE != -942 THEN RAISE; END IF;\n",
    "        END;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create chat_history table\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE chat_history (\n",
    "            id VARCHAR2(100) PRIMARY KEY,\n",
    "            thread_id VARCHAR2(100) NOT NULL,\n",
    "            role VARCHAR2(20) NOT NULL,\n",
    "            message CLOB NOT NULL,\n",
    "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create index on thread_id and timestamp for efficient retrieval\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX idx_thread_timestamp \n",
    "        ON chat_history(thread_id, timestamp)\n",
    "        TABLESPACE USERS\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"âœ… Table chat_history created successfully with index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c470ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def research_assistant_chat(user_query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the complete research assistant workflow with conversation history.\n",
    "    For each conversation turn:\n",
    "      - Stores the user's input and the assistant's output in Oracle along with a timestamp and thread_id.\n",
    "      - Retrieves and appends previous conversation history (ordered by timestamp) to the agent's input.\n",
    "    \n",
    "    If no thread_id is provided, a new conversation session is started.\n",
    "    \n",
    "    Returns:\n",
    "      tuple: (final_output, thread_id) where thread_id is the session identifier.\n",
    "    \"\"\"\n",
    "    # Generate a new thread id if not provided\n",
    "    if thread_id is None:\n",
    "        thread_id = str(uuid.uuid4())\n",
    "        print(f\"ðŸ“ New research conversation started with thread ID: {thread_id}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“ Continuing research conversation with thread ID: {thread_id}\")\n",
    "    \n",
    "    # --- Step 1: Store the new user query in Oracle ---\n",
    "    message_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': message_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'user',\n",
    "            'message': user_query\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    # --- Step 2: Retrieve full conversation history for context ---\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT role, message, timestamp\n",
    "            FROM chat_history\n",
    "            WHERE thread_id = :thread_id\n",
    "            ORDER BY timestamp ASC\n",
    "        \"\"\", {'thread_id': thread_id})\n",
    "        \n",
    "        chat_history = cur.fetchall()\n",
    "    \n",
    "    conversation_context = \"\"\n",
    "    for entry in chat_history:\n",
    "        role, message, timestamp = entry\n",
    "        if role == \"user\":\n",
    "            conversation_context += f\"User: {message}\\n\"\n",
    "        else:\n",
    "            conversation_context += f\"Assistant: {message}\\n\"\n",
    "    \n",
    "    # --- Step 3: Run the orchestrator agent with the conversation context ---\n",
    "    with trace(\"Research Orchestrator\"):\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, conversation_context)\n",
    "    \n",
    "    # Print intermediate processing steps for debugging/transparency\n",
    "    print(\"\\n--- Research Orchestrator Processing Steps ---\")\n",
    "    for item in orchestrator_result.new_items:\n",
    "        if isinstance(item, MessageOutputItem):\n",
    "            text = ItemHelpers.text_message_output(item)\n",
    "            if text:\n",
    "                print(f\"  - Information gathering step: {text}\")\n",
    "    \n",
    "    # --- Step 4: Run the synthesizer agent to produce a cohesive response ---\n",
    "    synthesizer_result = await Runner.run(\n",
    "        synthesizer_agent, orchestrator_result.to_input_list()\n",
    "    )\n",
    "    \n",
    "    # --- Step 5: Store the assistant's final output in Oracle ---\n",
    "    response_id = str(uuid.uuid4())\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO chat_history (id, thread_id, role, message, timestamp)\n",
    "            VALUES (:id, :thread_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "        \"\"\", {\n",
    "            'id': response_id,\n",
    "            'thread_id': thread_id,\n",
    "            'role': 'assistant',\n",
    "            'message': synthesizer_result.final_output\n",
    "        })\n",
    "        conn.commit()\n",
    "    \n",
    "    print(f\"\\n\\n--- Final Research Response ---\\n{synthesizer_result.final_output}\\n\")\n",
    "    \n",
    "    return synthesizer_result.final_output, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_research_assistant_chat(query, thread_id=None):\n",
    "    \"\"\"\n",
    "    Run the research assistant synchronously.\n",
    "    Optionally, a thread_id can be provided to continue an existing conversation.\n",
    "    Returns a tuple (final_output, thread_id).\n",
    "    \"\"\"\n",
    "    # Create a new event loop\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    \n",
    "    # Run the async function and get the result\n",
    "    result, thread_id = loop.run_until_complete(\n",
    "        research_assistant_chat(query, thread_id=thread_id)\n",
    "    )\n",
    "    \n",
    "    # Clean up the loop\n",
    "    loop.close()\n",
    "    \n",
    "    return result, thread_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_chat_session():\n",
    "    \"\"\"\n",
    "    Launches a research chat session that continues until the user enters 'q', 'exit', or 'quit'.\n",
    "    The session uses a persistent thread_id to preserve conversation history.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”¬ Starting Research Paper Assistant Chat\")\n",
    "    print(\"Type 'q', 'exit' or 'quit' to exit.\\n\")\n",
    "    \n",
    "    session_thread_id = None\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"What research topic can I help you with today? \")\n",
    "        \n",
    "        if query.lower() in [\"q\", \"exit\", \"quit\"]:\n",
    "            print(\"Exiting research chat session.\")\n",
    "            break\n",
    "        \n",
    "        response, session_thread_id = run_research_assistant_chat(\n",
    "            query, thread_id=session_thread_id\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nðŸ“š Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf336fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the research chat session\n",
    "research_chat_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af5bfe",
   "metadata": {},
   "source": [
    "## Session Memory with Oracle AI Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20956c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "from datetime import datetime\n",
    "import oracledb\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "class OracleSession:\n",
    "    \"\"\"Custom Oracle session implementation following the Session protocol\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        session_id: str, \n",
    "        connection,\n",
    "        table_name: str = \"chat_history\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Oracle session storage.\n",
    "        \n",
    "        Args:\n",
    "            session_id: Unique identifier for this conversation session\n",
    "            connection: Active oracledb connection object\n",
    "            table_name: Name of the Oracle table storing session data\n",
    "        \"\"\"\n",
    "        self.session_id = session_id\n",
    "        self.conn = connection\n",
    "        self.table_name = table_name\n",
    "    \n",
    "    async def get_items(self, limit: Optional[int] = None) -> List[dict]:\n",
    "        \"\"\"Retrieve conversation history for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                if limit:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                        FETCH FIRST :limit ROWS ONLY\n",
    "                    \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                else:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp ASC\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                items = []\n",
    "                for row in rows:\n",
    "                    # Deserialize JSON from CLOB\n",
    "                    message_clob = row[0]\n",
    "                    if message_clob:\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        items.append(json.loads(message_str))\n",
    "                \n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving items: {e}\")\n",
    "            return []\n",
    "    \n",
    "    async def add_items(self, items: List[dict]) -> None:\n",
    "        \"\"\"Store new items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                for item in items:\n",
    "                    item_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    # Serialize the entire item as JSON\n",
    "                    message_json = json.dumps(item)\n",
    "                    \n",
    "                    # Extract role if available, otherwise default to 'system'\n",
    "                    role = item.get('role', 'system')\n",
    "                    \n",
    "                    cur.execute(f\"\"\"\n",
    "                        INSERT INTO {self.table_name} (id, thread_id, role, message, timestamp)\n",
    "                        VALUES (:id, :session_id, :role, :message, CURRENT_TIMESTAMP)\n",
    "                    \"\"\", {\n",
    "                        'id': item_id,\n",
    "                        'session_id': self.session_id,\n",
    "                        'role': role,\n",
    "                        'message': message_json\n",
    "                    })\n",
    "                \n",
    "                self.conn.commit()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding items: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    async def pop_item(self, limit: Optional[int] = None) -> Optional[Union[dict, List[dict]]]:\n",
    "        \"\"\"\n",
    "        Remove and return the most recent item(s) for this session.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                # Pop a single most-recent item\n",
    "                if not limit or limit <= 1:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        SELECT id, message\n",
    "                        FROM {self.table_name}\n",
    "                        WHERE thread_id = :session_id\n",
    "                        ORDER BY timestamp DESC\n",
    "                        FETCH FIRST 1 ROW ONLY\n",
    "                    \"\"\", {'session_id': self.session_id})\n",
    "                    \n",
    "                    row = cur.fetchone()\n",
    "                    \n",
    "                    if row:\n",
    "                        item_id, message_clob = row\n",
    "                        message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                        item = json.loads(message_str)\n",
    "                        \n",
    "                        # Delete the item\n",
    "                        cur.execute(f\"\"\"\n",
    "                            DELETE FROM {self.table_name}\n",
    "                            WHERE id = :id\n",
    "                        \"\"\", {'id': item_id})\n",
    "                        \n",
    "                        self.conn.commit()\n",
    "                        return item\n",
    "                    \n",
    "                    return None\n",
    "                \n",
    "                # Pop multiple most-recent items\n",
    "                cur.execute(f\"\"\"\n",
    "                    SELECT id, message\n",
    "                    FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                    ORDER BY timestamp DESC\n",
    "                    FETCH FIRST :limit ROWS ONLY\n",
    "                \"\"\", {'session_id': self.session_id, 'limit': limit})\n",
    "                \n",
    "                rows = cur.fetchall()\n",
    "                \n",
    "                if not rows:\n",
    "                    return []\n",
    "                \n",
    "                items = []\n",
    "                ids_to_delete = []\n",
    "                \n",
    "                for row in rows:\n",
    "                    item_id, message_clob = row\n",
    "                    message_str = message_clob.read() if hasattr(message_clob, 'read') else str(message_clob)\n",
    "                    items.append(json.loads(message_str))\n",
    "                    ids_to_delete.append(item_id)\n",
    "                \n",
    "                # Delete all items\n",
    "                for item_id in ids_to_delete:\n",
    "                    cur.execute(f\"\"\"\n",
    "                        DELETE FROM {self.table_name}\n",
    "                        WHERE id = :id\n",
    "                    \"\"\", {'id': item_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                return items\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error popping item(s): {e}\")\n",
    "            self.conn.rollback()\n",
    "            return None if (not limit or limit <= 1) else []\n",
    "    \n",
    "    async def clear_session(self) -> None:\n",
    "        \"\"\"Clear all items for this session\"\"\"\n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                cur.execute(f\"\"\"\n",
    "                    DELETE FROM {self.table_name}\n",
    "                    WHERE thread_id = :session_id\n",
    "                \"\"\", {'session_id': self.session_id})\n",
    "                \n",
    "                self.conn.commit()\n",
    "                print(f\"âœ… Session {self.session_id} cleared successfully.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error clearing session: {e}\")\n",
    "            self.conn.rollback()\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Note: Connection is managed externally, so we don't close it here.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079aa0f1",
   "metadata": {},
   "source": [
    "Basic Example of an Agent with Session Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent\n",
    "research_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Research the topic and return the most relevant information.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Oracle session instance\n",
    "session = OracleSession(\n",
    "    session_id=\"conversation_123\", \n",
    "    connection=conn,\n",
    "    table_name=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Hi my name is Richmond, and I am a AI Engineer researching LLMs and Agent Memory\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea86e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second turn\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What is a paper that introduces the attention mechanism in LLMs?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c2920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third turn, the agent will remember the previous conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Who were the authors of the paper?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f955bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth turn - continuing the conversation\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What was the year of publication?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d64efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the conversation subject and ensure the agent does't remember the previous conversation\n",
    "# Specifiying pop without limit will remove the last item in the session\n",
    "await session.pop_item(limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth turn: The agent should not remember the conversations about the paper\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"What paper have we been talking about?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we limited the session to a few items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session\n",
    "await session.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2169c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we limited the session to 3 items, the agent should still remember our name at the introduction\n",
    "result_from_research_agent = await Runner.run(\n",
    "    starting_agent=research_agent,\n",
    "    input=\"Do you still remember my name?\",\n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {result_from_research_agent.final_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oracle_demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
